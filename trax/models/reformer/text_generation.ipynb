{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reformer: Text Generation",
      "provenance": [],
      "collapsed_sections": [
        "udDs_biH0n5U"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udDs_biH0n5U",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPY-OyyM0pSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        " https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psnUF-8c02o_",
        "colab_type": "text"
      },
      "source": [
        "# Reformer: Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lnRd_IoERdk",
        "colab_type": "text"
      },
      "source": [
        "On the main menu, click Runtime and select Change runtime type. Set \"TPU\" as the hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PluCmWbZIpJ",
        "colab_type": "code",
        "outputId": "3e08682a-a1e5-4496-90f3-65b54f5f6f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Grab newest JAX version.\n",
        "!pip install --upgrade -q jax==0.1.57 jaxlib==0.1.37\n",
        "\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grpc://10.4.147.210:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiPdBenoZwH6",
        "colab_type": "code",
        "outputId": "c579ba94-43a2-451d-a138-f3493cad85fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# TODO(kitaev): decide which commit we're pinning\n",
        "!pip install --upgrade -q sentencepiece\n",
        "!pip install --upgrade -q gin git+https://github.com/google/trax.git@d2c5b84b8db53888013c56980ccbafba077ee8f2\n",
        "\n",
        "from tensorflow.compat.v1.io.gfile import GFile\n",
        "import gin\n",
        "import os\n",
        "import jax\n",
        "import trax\n",
        "from trax.supervised import inputs\n",
        "\n",
        "import numpy as onp\n",
        "import jax.numpy as np\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for trax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FQ89jHCYfhpg"
      },
      "source": [
        "## Setting up data and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMntV3H-6OR0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "9d1b2c8b-b12d-469d-f244-f6980363eed5"
      },
      "source": [
        "# Load a BPE vocabulaary with 320 types. This mostly consists of single letters\n",
        "# and pairs of letters, but it has some common words and word pieces, too.\n",
        "!gsutil cp gs://trax-ml/reformer/cp.320.* .\n",
        "\n",
        "TOKENIZER = SentencePieceProcessor()\n",
        "TOKENIZER.load('cp.320.model')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://trax-ml/reformer/cp.320.model...\n",
            "Copying gs://trax-ml/reformer/cp.320.vocab...\n",
            "/ [2 files][239.0 KiB/239.0 KiB]                                                \n",
            "Operation completed over 2 objects/239.0 KiB.                                    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYSOVGR47LVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import a copy of \"Crime and Punishment\", by Fyodor Dostoevsky\n",
        "with GFile('gs://trax-ml/reformer/crime-and-punishment-2554.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# The file read above includes metadata and licensing information.\n",
        "# For training our language model, we will only use the actual novel text.\n",
        "start = text.find('CRIME AND PUNISHMENT')  # skip header\n",
        "start = text.find('CRIME AND PUNISHMENT', start + 1)  # skip header\n",
        "start = text.find('CRIME AND PUNISHMENT', start + 1)  # skip translator preface\n",
        "end = text.rfind('End of Project')  # skip extra text at the end\n",
        "text = text[start:end].strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnJzxSi_77zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "IDS = TOKENIZER.EncodeAsIds(text)\n",
        "IDS = onp.asarray(IDS, dtype=onp.int32)\n",
        "PAD_AMOUNT = 512 * 1024 - len(IDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAwmpS220ub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dbf97ac0-6144-41ff-d64b-aaa098cfae03"
      },
      "source": [
        "# Now we set up the data pipeline.\n",
        "# To prevent the model from memorizing the dataset using the position embeddings\n",
        "# we randomly select how much padding to put before the text vs. after it.\n",
        "def my_inputs(n_devices):\n",
        "  while True:\n",
        "    inputs = []\n",
        "    mask = []\n",
        "    pad_amounts = onp.random.choice(PAD_AMOUNT, n_devices)\n",
        "    for i in range(n_devices):\n",
        "      inputs.append(onp.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
        "                            mode='constant'))\n",
        "      mask.append(onp.pad(onp.ones_like(IDS, dtype=onp.float32),\n",
        "                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
        "                          mode='constant'))\n",
        "    inputs = onp.stack(inputs)\n",
        "    mask = onp.stack(mask)\n",
        "    yield (inputs, inputs, mask)\n",
        "\n",
        "print(\"Token count in text:\", IDS.shape[0])\n",
        "print(\"Size of batch, across all devices:\",\n",
        "      next(my_inputs(trax.math.device_count()))[0].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token count in text: 513812\n",
            "Size of batch, across all devices: (8, 524288)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei90LdK024r_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bindings = [\"\"\"\n",
        "import trax.layers\n",
        "import trax.models\n",
        "import trax.optimizers\n",
        "import trax.supervised.inputs\n",
        "import trax.supervised.trainer_lib\n",
        "\n",
        "# Parameters that will vary between experiments:\n",
        "# ==============================================================================\n",
        "train.model = @trax.models.ReformerLM\n",
        "n_heads = 2\n",
        "attn_type = [\n",
        "  @TimeBinCausalAttention,\n",
        "  @LSHCausalAttention,  \n",
        "  @TimeBinCausalAttention,\n",
        "  @LSHCausalAttention,\n",
        "  @TimeBinCausalAttention,\n",
        "  @LSHCausalAttention,\n",
        "  ]\n",
        "share_qk = False  # LSHCausalAttention ignores this flag and always shares q & k\n",
        "attn_kv = 64\n",
        "n_layers = 6\n",
        "dropout = 0.05\n",
        "n_tokens = 524288\n",
        "\n",
        "# Parameters for MultifactorSchedule:\n",
        "# ==============================================================================\n",
        "MultifactorSchedule.constant = 0.01\n",
        "MultifactorSchedule.factors = 'constant * linear_warmup * cosine_decay'\n",
        "MultifactorSchedule.warmup_steps = 60\n",
        "MultifactorSchedule.steps_per_cycle = 540\n",
        "\n",
        "# Parameters for Adam:\n",
        "# ==============================================================================\n",
        "Adam.weight_decay_rate=0.0\n",
        "Adam.b1 = 0.86\n",
        "Adam.b2 = 0.92\n",
        "Adam.eps = 1e-9\n",
        "\n",
        "# Parameters for TimeBinCausalAttention:\n",
        "# ==============================================================================\n",
        "TimeBinCausalAttention.bin_length = 64\n",
        "TimeBinCausalAttention.dropout = 0.05\n",
        "TimeBinCausalAttention.n_bins = None\n",
        "TimeBinCausalAttention.share_qk = %share_qk\n",
        "\n",
        "# Parameters for LSHCausalAttention:\n",
        "# ==============================================================================\n",
        "LSHCausalAttention.allow_duplicate_attention = False\n",
        "LSHCausalAttention.attend_across_buckets = True\n",
        "LSHCausalAttention.rehash_each_round = True\n",
        "LSHCausalAttention.data_rotation = False\n",
        "LSHCausalAttention.n_bins = 4096\n",
        "LSHCausalAttention.n_buckets = 8192\n",
        "LSHCausalAttention.factorize_hash = [64, 128]\n",
        "LSHCausalAttention.n_hashes = 1\n",
        "LSHCausalAttention.one_rng = False\n",
        "LSHCausalAttention.hard_k = 0\n",
        "LSHCausalAttention.dropout = 0.0\n",
        "LSHCausalAttention.drop_for_hash_rate = 0.0\n",
        "LSHCausalAttention.max_len_for_inference = 2048\n",
        "LSHCausalAttention.bucket_capacity_for_inference = 64\n",
        "\n",
        "# Parameters for ReformerLM:\n",
        "# ==============================================================================\n",
        "ReformerLM.attention_type = %attn_type\n",
        "ReformerLM.d_attention_key = %attn_kv\n",
        "ReformerLM.d_attention_value = %attn_kv\n",
        "ReformerLM.d_model = 256\n",
        "ReformerLM.d_ff = 512\n",
        "ReformerLM.dropout = %dropout\n",
        "ReformerLM.ff_activation = @trax.layers.Relu\n",
        "ReformerLM.max_len = %n_tokens\n",
        "ReformerLM.mode = 'train'\n",
        "ReformerLM.n_heads = %n_heads\n",
        "ReformerLM.n_layers = %n_layers\n",
        "ReformerLM.vocab_size = 320\n",
        "ReformerLM.share_qk = %share_qk\n",
        "ReformerLM.axial_pos_shape = (512, 1024)\n",
        "# ReformerLM.axial_pos_shape = (492, 1024)\n",
        "ReformerLM.d_axial_pos_embs= (64, 192)\n",
        "\"\"\"]\n",
        "\n",
        "gin.parse_config_files_and_bindings(\n",
        "    [], bindings, finalize_config=False, skip_unknown=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGGt0WaT3a-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model with Trainer.\n",
        "output_dir = os.path.expanduser('~/train_dir/')\n",
        "!rm -f ~/train_dir/model.pkl  # Remove old model\n",
        "trainer = trax.supervised.Trainer(\n",
        "    model=trax.models.ReformerLM,\n",
        "    loss_fn=trax.layers.CrossEntropyLossScalar,\n",
        "    optimizer=trax.optimizers.Adam,\n",
        "    lr_schedule=trax.lr.MultifactorSchedule,\n",
        "    inputs=trax.supervised.inputs.Inputs(my_inputs),\n",
        "    output_dir=output_dir,\n",
        "    has_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6VQkmKO3a1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "e8aa78d6-73ab-402d-c731-cc82e510e9eb"
      },
      "source": [
        "# Run one training step, to make sure we fit in memory.\n",
        "# The first time trainer.train_epoch is called, it will JIT the entire network\n",
        "# architecture, which takes around 2 minutes. The JIT-compiled model is saved,\n",
        "# so subsequent runs will be much faster than the first.\n",
        "trainer.train_epoch(n_steps=1, n_eval_steps=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Ran 1 train steps in 121.71 secs\n",
            "Step      1: Evaluation\n",
            "Step      1: train                   accuracy |  0.00618636\n",
            "Step      1: train                       loss |  6.35527897\n",
            "Step      1: train         neg_log_perplexity | -6.35527897\n",
            "Step      1: train weights_per_batch_per_core |  513812.00000000\n",
            "Step      1: eval                    accuracy |  0.00625156\n",
            "Step      1: eval                        loss |  6.35493374\n",
            "Step      1: eval          neg_log_perplexity | -6.35493374\n",
            "Step      1: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step      1: Finished evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFnX4G6z3asD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebbd0f4c-504c-4e5a-df42-ec74bf0d3760"
      },
      "source": [
        "# Train for 600 steps total.\n",
        "# The language model won't be exceptionally good when trained for so few steps\n",
        "# and without regularization, but we can still try to sample from it at this\n",
        "# point.\n",
        "trainer.train_epoch(n_steps=9, n_eval_steps=1)\n",
        "for _ in range(59):\n",
        "  trainer.train_epoch(n_steps=10, n_eval_steps=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step     10: Ran 9 train steps in 156.78 secs\n",
            "Step     10: Evaluation\n",
            "Step     10: train                   accuracy |  0.04241240\n",
            "Step     10: train                       loss |  5.20038080\n",
            "Step     10: train         neg_log_perplexity | -5.20038080\n",
            "Step     10: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     10: eval                    accuracy |  0.04253574\n",
            "Step     10: eval                        loss |  5.20046568\n",
            "Step     10: eval          neg_log_perplexity | -5.20046568\n",
            "Step     10: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     10: Finished evaluation\n",
            "\n",
            "Step     20: Ran 10 train steps in 38.02 secs\n",
            "Step     20: Evaluation\n",
            "Step     20: train                   accuracy |  0.09202103\n",
            "Step     20: train                       loss |  4.53193760\n",
            "Step     20: train         neg_log_perplexity | -4.53193760\n",
            "Step     20: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     20: eval                    accuracy |  0.09187457\n",
            "Step     20: eval                        loss |  4.53243113\n",
            "Step     20: eval          neg_log_perplexity | -4.53243113\n",
            "Step     20: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     20: Finished evaluation\n",
            "\n",
            "Step     30: Ran 10 train steps in 38.16 secs\n",
            "Step     30: Evaluation\n",
            "Step     30: train                   accuracy |  0.12118077\n",
            "Step     30: train                       loss |  4.01824951\n",
            "Step     30: train         neg_log_perplexity | -4.01824951\n",
            "Step     30: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     30: eval                    accuracy |  0.12115180\n",
            "Step     30: eval                        loss |  4.01827097\n",
            "Step     30: eval          neg_log_perplexity | -4.01827097\n",
            "Step     30: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     30: Finished evaluation\n",
            "\n",
            "Step     40: Ran 10 train steps in 38.36 secs\n",
            "Step     40: Evaluation\n",
            "Step     40: train                   accuracy |  0.12658571\n",
            "Step     40: train                       loss |  3.83307314\n",
            "Step     40: train         neg_log_perplexity | -3.83307314\n",
            "Step     40: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     40: eval                    accuracy |  0.12655383\n",
            "Step     40: eval                        loss |  3.83299899\n",
            "Step     40: eval          neg_log_perplexity | -3.83299899\n",
            "Step     40: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     40: Finished evaluation\n",
            "\n",
            "Step     50: Ran 10 train steps in 38.30 secs\n",
            "Step     50: Evaluation\n",
            "Step     50: train                   accuracy |  0.12996510\n",
            "Step     50: train                       loss |  3.70916224\n",
            "Step     50: train         neg_log_perplexity | -3.70916224\n",
            "Step     50: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     50: eval                    accuracy |  0.12966003\n",
            "Step     50: eval                        loss |  3.70910573\n",
            "Step     50: eval          neg_log_perplexity | -3.70910573\n",
            "Step     50: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     50: Finished evaluation\n",
            "\n",
            "Step     60: Ran 10 train steps in 38.46 secs\n",
            "Step     60: Evaluation\n",
            "Step     60: train                   accuracy |  0.13194540\n",
            "Step     60: train                       loss |  3.71005940\n",
            "Step     60: train         neg_log_perplexity | -3.71005940\n",
            "Step     60: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     60: eval                    accuracy |  0.13186778\n",
            "Step     60: eval                        loss |  3.71005678\n",
            "Step     60: eval          neg_log_perplexity | -3.71005678\n",
            "Step     60: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     60: Finished evaluation\n",
            "\n",
            "Step     70: Ran 10 train steps in 38.66 secs\n",
            "Step     70: Evaluation\n",
            "Step     70: train                   accuracy |  0.14217921\n",
            "Step     70: train                       loss |  3.60436940\n",
            "Step     70: train         neg_log_perplexity | -3.60436940\n",
            "Step     70: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     70: eval                    accuracy |  0.14199334\n",
            "Step     70: eval                        loss |  3.60463858\n",
            "Step     70: eval          neg_log_perplexity | -3.60463858\n",
            "Step     70: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     70: Finished evaluation\n",
            "\n",
            "Step     80: Ran 10 train steps in 38.36 secs\n",
            "Step     80: Evaluation\n",
            "Step     80: train                   accuracy |  0.14551042\n",
            "Step     80: train                       loss |  3.56646490\n",
            "Step     80: train         neg_log_perplexity | -3.56646490\n",
            "Step     80: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     80: eval                    accuracy |  0.14550604\n",
            "Step     80: eval                        loss |  3.56630063\n",
            "Step     80: eval          neg_log_perplexity | -3.56630063\n",
            "Step     80: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     80: Finished evaluation\n",
            "\n",
            "Step     90: Ran 10 train steps in 38.18 secs\n",
            "Step     90: Evaluation\n",
            "Step     90: train                   accuracy |  0.14936276\n",
            "Step     90: train                       loss |  3.54491115\n",
            "Step     90: train         neg_log_perplexity | -3.54491115\n",
            "Step     90: train weights_per_batch_per_core |  513812.00000000\n",
            "Step     90: eval                    accuracy |  0.14936155\n",
            "Step     90: eval                        loss |  3.54492354\n",
            "Step     90: eval          neg_log_perplexity | -3.54492354\n",
            "Step     90: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step     90: Finished evaluation\n",
            "\n",
            "Step    100: Ran 10 train steps in 38.35 secs\n",
            "Step    100: Evaluation\n",
            "Step    100: train                   accuracy |  0.15310319\n",
            "Step    100: train                       loss |  3.51188564\n",
            "Step    100: train         neg_log_perplexity | -3.51188564\n",
            "Step    100: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    100: eval                    accuracy |  0.15321997\n",
            "Step    100: eval                        loss |  3.51157522\n",
            "Step    100: eval          neg_log_perplexity | -3.51157522\n",
            "Step    100: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    100: Finished evaluation\n",
            "\n",
            "Step    110: Ran 10 train steps in 38.30 secs\n",
            "Step    110: Evaluation\n",
            "Step    110: train                   accuracy |  0.15789312\n",
            "Step    110: train                       loss |  3.50535107\n",
            "Step    110: train         neg_log_perplexity | -3.50535107\n",
            "Step    110: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    110: eval                    accuracy |  0.15797681\n",
            "Step    110: eval                        loss |  3.50551057\n",
            "Step    110: eval          neg_log_perplexity | -3.50551057\n",
            "Step    110: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    110: Finished evaluation\n",
            "\n",
            "Step    120: Ran 10 train steps in 38.26 secs\n",
            "Step    120: Evaluation\n",
            "Step    120: train                   accuracy |  0.16504432\n",
            "Step    120: train                       loss |  3.46542120\n",
            "Step    120: train         neg_log_perplexity | -3.46542120\n",
            "Step    120: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    120: eval                    accuracy |  0.16516061\n",
            "Step    120: eval                        loss |  3.46581912\n",
            "Step    120: eval          neg_log_perplexity | -3.46581912\n",
            "Step    120: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    120: Finished evaluation\n",
            "\n",
            "Step    130: Ran 10 train steps in 38.30 secs\n",
            "Step    130: Evaluation\n",
            "Step    130: train                   accuracy |  0.16951096\n",
            "Step    130: train                       loss |  3.43826771\n",
            "Step    130: train         neg_log_perplexity | -3.43826771\n",
            "Step    130: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    130: eval                    accuracy |  0.16972211\n",
            "Step    130: eval                        loss |  3.43778539\n",
            "Step    130: eval          neg_log_perplexity | -3.43778539\n",
            "Step    130: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    130: Finished evaluation\n",
            "\n",
            "Step    140: Ran 10 train steps in 38.39 secs\n",
            "Step    140: Evaluation\n",
            "Step    140: train                   accuracy |  0.17515868\n",
            "Step    140: train                       loss |  3.41301584\n",
            "Step    140: train         neg_log_perplexity | -3.41301584\n",
            "Step    140: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    140: eval                    accuracy |  0.17521316\n",
            "Step    140: eval                        loss |  3.41270542\n",
            "Step    140: eval          neg_log_perplexity | -3.41270542\n",
            "Step    140: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    140: Finished evaluation\n",
            "\n",
            "Step    150: Ran 10 train steps in 38.36 secs\n",
            "Step    150: Evaluation\n",
            "Step    150: train                   accuracy |  0.18161094\n",
            "Step    150: train                       loss |  3.38249159\n",
            "Step    150: train         neg_log_perplexity | -3.38249159\n",
            "Step    150: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    150: eval                    accuracy |  0.18131000\n",
            "Step    150: eval                        loss |  3.38394833\n",
            "Step    150: eval          neg_log_perplexity | -3.38394833\n",
            "Step    150: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    150: Finished evaluation\n",
            "\n",
            "Step    160: Ran 10 train steps in 38.27 secs\n",
            "Step    160: Evaluation\n",
            "Step    160: train                   accuracy |  0.18914117\n",
            "Step    160: train                       loss |  3.34644175\n",
            "Step    160: train         neg_log_perplexity | -3.34644175\n",
            "Step    160: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    160: eval                    accuracy |  0.18884680\n",
            "Step    160: eval                        loss |  3.34801435\n",
            "Step    160: eval          neg_log_perplexity | -3.34801435\n",
            "Step    160: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    160: Finished evaluation\n",
            "\n",
            "Step    170: Ran 10 train steps in 38.37 secs\n",
            "Step    170: Evaluation\n",
            "Step    170: train                   accuracy |  0.18946037\n",
            "Step    170: train                       loss |  3.33758736\n",
            "Step    170: train         neg_log_perplexity | -3.33758736\n",
            "Step    170: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    170: eval                    accuracy |  0.18906820\n",
            "Step    170: eval                        loss |  3.33982468\n",
            "Step    170: eval          neg_log_perplexity | -3.33982468\n",
            "Step    170: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    170: Finished evaluation\n",
            "\n",
            "Step    180: Ran 10 train steps in 38.51 secs\n",
            "Step    180: Evaluation\n",
            "Step    180: train                   accuracy |  0.20083028\n",
            "Step    180: train                       loss |  3.28433609\n",
            "Step    180: train         neg_log_perplexity | -3.28433609\n",
            "Step    180: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    180: eval                    accuracy |  0.20154187\n",
            "Step    180: eval                        loss |  3.28076863\n",
            "Step    180: eval          neg_log_perplexity | -3.28076863\n",
            "Step    180: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    180: Finished evaluation\n",
            "\n",
            "Step    190: Ran 10 train steps in 38.25 secs\n",
            "Step    190: Evaluation\n",
            "Step    190: train                   accuracy |  0.20729567\n",
            "Step    190: train                       loss |  3.24777555\n",
            "Step    190: train         neg_log_perplexity | -3.24777555\n",
            "Step    190: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    190: eval                    accuracy |  0.20712441\n",
            "Step    190: eval                        loss |  3.24886823\n",
            "Step    190: eval          neg_log_perplexity | -3.24886823\n",
            "Step    190: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    190: Finished evaluation\n",
            "\n",
            "Step    200: Ran 10 train steps in 38.41 secs\n",
            "Step    200: Evaluation\n",
            "Step    200: train                   accuracy |  0.21815932\n",
            "Step    200: train                       loss |  3.19416189\n",
            "Step    200: train         neg_log_perplexity | -3.19416189\n",
            "Step    200: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    200: eval                    accuracy |  0.21840090\n",
            "Step    200: eval                        loss |  3.19296002\n",
            "Step    200: eval          neg_log_perplexity | -3.19296002\n",
            "Step    200: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    200: Finished evaluation\n",
            "\n",
            "Step    210: Ran 10 train steps in 38.35 secs\n",
            "Step    210: Evaluation\n",
            "Step    210: train                   accuracy |  0.23925775\n",
            "Step    210: train                       loss |  3.10201025\n",
            "Step    210: train         neg_log_perplexity | -3.10201025\n",
            "Step    210: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    210: eval                    accuracy |  0.23877922\n",
            "Step    210: eval                        loss |  3.10330009\n",
            "Step    210: eval          neg_log_perplexity | -3.10330009\n",
            "Step    210: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    210: Finished evaluation\n",
            "\n",
            "Step    220: Ran 10 train steps in 38.23 secs\n",
            "Step    220: Evaluation\n",
            "Step    220: train                   accuracy |  0.27684277\n",
            "Step    220: train                       loss |  2.93406653\n",
            "Step    220: train         neg_log_perplexity | -2.93406653\n",
            "Step    220: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    220: eval                    accuracy |  0.27729940\n",
            "Step    220: eval                        loss |  2.93183708\n",
            "Step    220: eval          neg_log_perplexity | -2.93183708\n",
            "Step    220: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    220: Finished evaluation\n",
            "\n",
            "Step    230: Ran 10 train steps in 38.40 secs\n",
            "Step    230: Evaluation\n",
            "Step    230: train                   accuracy |  0.31048056\n",
            "Step    230: train                       loss |  2.77878785\n",
            "Step    230: train         neg_log_perplexity | -2.77878785\n",
            "Step    230: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    230: eval                    accuracy |  0.31028473\n",
            "Step    230: eval                        loss |  2.77765465\n",
            "Step    230: eval          neg_log_perplexity | -2.77765465\n",
            "Step    230: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    230: Finished evaluation\n",
            "\n",
            "Step    240: Ran 10 train steps in 38.45 secs\n",
            "Step    240: Evaluation\n",
            "Step    240: train                   accuracy |  0.34180793\n",
            "Step    240: train                       loss |  2.62886333\n",
            "Step    240: train         neg_log_perplexity | -2.62886333\n",
            "Step    240: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    240: eval                    accuracy |  0.34177655\n",
            "Step    240: eval                        loss |  2.62978983\n",
            "Step    240: eval          neg_log_perplexity | -2.62978983\n",
            "Step    240: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    240: Finished evaluation\n",
            "\n",
            "Step    250: Ran 10 train steps in 38.39 secs\n",
            "Step    250: Evaluation\n",
            "Step    250: train                   accuracy |  0.36227092\n",
            "Step    250: train                       loss |  2.52726173\n",
            "Step    250: train         neg_log_perplexity | -2.52726173\n",
            "Step    250: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    250: eval                    accuracy |  0.36179578\n",
            "Step    250: eval                        loss |  2.52977276\n",
            "Step    250: eval          neg_log_perplexity | -2.52977276\n",
            "Step    250: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    250: Finished evaluation\n",
            "\n",
            "Step    260: Ran 10 train steps in 38.53 secs\n",
            "Step    260: Evaluation\n",
            "Step    260: train                   accuracy |  0.38025194\n",
            "Step    260: train                       loss |  2.44068003\n",
            "Step    260: train         neg_log_perplexity | -2.44068003\n",
            "Step    260: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    260: eval                    accuracy |  0.38018209\n",
            "Step    260: eval                        loss |  2.44111848\n",
            "Step    260: eval          neg_log_perplexity | -2.44111848\n",
            "Step    260: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    260: Finished evaluation\n",
            "\n",
            "Step    270: Ran 10 train steps in 38.52 secs\n",
            "Step    270: Evaluation\n",
            "Step    270: train                   accuracy |  0.39666599\n",
            "Step    270: train                       loss |  2.35048246\n",
            "Step    270: train         neg_log_perplexity | -2.35048246\n",
            "Step    270: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    270: eval                    accuracy |  0.39734626\n",
            "Step    270: eval                        loss |  2.34852147\n",
            "Step    270: eval          neg_log_perplexity | -2.34852147\n",
            "Step    270: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    270: Finished evaluation\n",
            "\n",
            "Step    280: Ran 10 train steps in 38.45 secs\n",
            "Step    280: Evaluation\n",
            "Step    280: train                   accuracy |  0.41244611\n",
            "Step    280: train                       loss |  2.27843142\n",
            "Step    280: train         neg_log_perplexity | -2.27843142\n",
            "Step    280: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    280: eval                    accuracy |  0.41238114\n",
            "Step    280: eval                        loss |  2.27858090\n",
            "Step    280: eval          neg_log_perplexity | -2.27858090\n",
            "Step    280: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    280: Finished evaluation\n",
            "\n",
            "Step    290: Ran 10 train steps in 38.32 secs\n",
            "Step    290: Evaluation\n",
            "Step    290: train                   accuracy |  0.42529249\n",
            "Step    290: train                       loss |  2.21635628\n",
            "Step    290: train         neg_log_perplexity | -2.21635628\n",
            "Step    290: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    290: eval                    accuracy |  0.42605788\n",
            "Step    290: eval                        loss |  2.21302652\n",
            "Step    290: eval          neg_log_perplexity | -2.21302652\n",
            "Step    290: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    290: Finished evaluation\n",
            "\n",
            "Step    300: Ran 10 train steps in 38.22 secs\n",
            "Step    300: Evaluation\n",
            "Step    300: train                   accuracy |  0.43432158\n",
            "Step    300: train                       loss |  2.16690159\n",
            "Step    300: train         neg_log_perplexity | -2.16690159\n",
            "Step    300: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    300: eval                    accuracy |  0.43578735\n",
            "Step    300: eval                        loss |  2.16021967\n",
            "Step    300: eval          neg_log_perplexity | -2.16021967\n",
            "Step    300: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    300: Finished evaluation\n",
            "\n",
            "Step    310: Ran 10 train steps in 38.41 secs\n",
            "Step    310: Evaluation\n",
            "Step    310: train                   accuracy |  0.44910935\n",
            "Step    310: train                       loss |  2.09793997\n",
            "Step    310: train         neg_log_perplexity | -2.09793997\n",
            "Step    310: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    310: eval                    accuracy |  0.44867682\n",
            "Step    310: eval                        loss |  2.09913039\n",
            "Step    310: eval          neg_log_perplexity | -2.09913039\n",
            "Step    310: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    310: Finished evaluation\n",
            "\n",
            "Step    320: Ran 10 train steps in 38.36 secs\n",
            "Step    320: Evaluation\n",
            "Step    320: train                   accuracy |  0.45838171\n",
            "Step    320: train                       loss |  2.05012321\n",
            "Step    320: train         neg_log_perplexity | -2.05012321\n",
            "Step    320: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    320: eval                    accuracy |  0.45951974\n",
            "Step    320: eval                        loss |  2.04421926\n",
            "Step    320: eval          neg_log_perplexity | -2.04421926\n",
            "Step    320: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    320: Finished evaluation\n",
            "\n",
            "Step    330: Ran 10 train steps in 38.26 secs\n",
            "Step    330: Evaluation\n",
            "Step    330: train                   accuracy |  0.46813646\n",
            "Step    330: train                       loss |  2.00718975\n",
            "Step    330: train         neg_log_perplexity | -2.00718975\n",
            "Step    330: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    330: eval                    accuracy |  0.46824473\n",
            "Step    330: eval                        loss |  2.00575233\n",
            "Step    330: eval          neg_log_perplexity | -2.00575233\n",
            "Step    330: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    330: Finished evaluation\n",
            "\n",
            "Step    340: Ran 10 train steps in 38.48 secs\n",
            "Step    340: Evaluation\n",
            "Step    340: train                   accuracy |  0.47927916\n",
            "Step    340: train                       loss |  1.95474780\n",
            "Step    340: train         neg_log_perplexity | -1.95474780\n",
            "Step    340: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    340: eval                    accuracy |  0.47910643\n",
            "Step    340: eval                        loss |  1.95531654\n",
            "Step    340: eval          neg_log_perplexity | -1.95531654\n",
            "Step    340: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    340: Finished evaluation\n",
            "\n",
            "Step    350: Ran 10 train steps in 38.57 secs\n",
            "Step    350: Evaluation\n",
            "Step    350: train                   accuracy |  0.49024814\n",
            "Step    350: train                       loss |  1.90189004\n",
            "Step    350: train         neg_log_perplexity | -1.90189004\n",
            "Step    350: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    350: eval                    accuracy |  0.49079457\n",
            "Step    350: eval                        loss |  1.89922607\n",
            "Step    350: eval          neg_log_perplexity | -1.89922607\n",
            "Step    350: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    350: Finished evaluation\n",
            "\n",
            "Step    360: Ran 10 train steps in 38.44 secs\n",
            "Step    360: Evaluation\n",
            "Step    360: train                   accuracy |  0.49809638\n",
            "Step    360: train                       loss |  1.86697364\n",
            "Step    360: train         neg_log_perplexity | -1.86697364\n",
            "Step    360: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    360: eval                    accuracy |  0.49827686\n",
            "Step    360: eval                        loss |  1.86666739\n",
            "Step    360: eval          neg_log_perplexity | -1.86666739\n",
            "Step    360: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    360: Finished evaluation\n",
            "\n",
            "Step    370: Ran 10 train steps in 38.35 secs\n",
            "Step    370: Evaluation\n",
            "Step    370: train                   accuracy |  0.50878048\n",
            "Step    370: train                       loss |  1.81789327\n",
            "Step    370: train         neg_log_perplexity | -1.81789327\n",
            "Step    370: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    370: eval                    accuracy |  0.50791222\n",
            "Step    370: eval                        loss |  1.82073021\n",
            "Step    370: eval          neg_log_perplexity | -1.82073021\n",
            "Step    370: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    370: Finished evaluation\n",
            "\n",
            "Step    380: Ran 10 train steps in 38.53 secs\n",
            "Step    380: Evaluation\n",
            "Step    380: train                   accuracy |  0.51547748\n",
            "Step    380: train                       loss |  1.78634787\n",
            "Step    380: train         neg_log_perplexity | -1.78634787\n",
            "Step    380: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    380: eval                    accuracy |  0.51640826\n",
            "Step    380: eval                        loss |  1.78220618\n",
            "Step    380: eval          neg_log_perplexity | -1.78220618\n",
            "Step    380: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    380: Finished evaluation\n",
            "\n",
            "Step    390: Ran 10 train steps in 38.27 secs\n",
            "Step    390: Evaluation\n",
            "Step    390: train                   accuracy |  0.52406573\n",
            "Step    390: train                       loss |  1.75017607\n",
            "Step    390: train         neg_log_perplexity | -1.75017607\n",
            "Step    390: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    390: eval                    accuracy |  0.52294838\n",
            "Step    390: eval                        loss |  1.75421965\n",
            "Step    390: eval          neg_log_perplexity | -1.75421965\n",
            "Step    390: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    390: Finished evaluation\n",
            "\n",
            "Step    400: Ran 10 train steps in 38.53 secs\n",
            "Step    400: Evaluation\n",
            "Step    400: train                   accuracy |  0.53233361\n",
            "Step    400: train                       loss |  1.71196246\n",
            "Step    400: train         neg_log_perplexity | -1.71196246\n",
            "Step    400: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    400: eval                    accuracy |  0.53160131\n",
            "Step    400: eval                        loss |  1.71560740\n",
            "Step    400: eval          neg_log_perplexity | -1.71560740\n",
            "Step    400: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    400: Finished evaluation\n",
            "\n",
            "Step    410: Ran 10 train steps in 38.44 secs\n",
            "Step    410: Evaluation\n",
            "Step    410: train                   accuracy |  0.53623068\n",
            "Step    410: train                       loss |  1.69503081\n",
            "Step    410: train         neg_log_perplexity | -1.69503081\n",
            "Step    410: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    410: eval                    accuracy |  0.53741449\n",
            "Step    410: eval                        loss |  1.69018638\n",
            "Step    410: eval          neg_log_perplexity | -1.69018638\n",
            "Step    410: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    410: Finished evaluation\n",
            "\n",
            "Step    420: Ran 10 train steps in 38.42 secs\n",
            "Step    420: Evaluation\n",
            "Step    420: train                   accuracy |  0.54387772\n",
            "Step    420: train                       loss |  1.66493344\n",
            "Step    420: train         neg_log_perplexity | -1.66493344\n",
            "Step    420: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    420: eval                    accuracy |  0.54680121\n",
            "Step    420: eval                        loss |  1.65415776\n",
            "Step    420: eval          neg_log_perplexity | -1.65415776\n",
            "Step    420: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    420: Finished evaluation\n",
            "\n",
            "Step    430: Ran 10 train steps in 38.47 secs\n",
            "Step    430: Evaluation\n",
            "Step    430: train                   accuracy |  0.55261457\n",
            "Step    430: train                       loss |  1.62846279\n",
            "Step    430: train         neg_log_perplexity | -1.62846279\n",
            "Step    430: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    430: eval                    accuracy |  0.55105519\n",
            "Step    430: eval                        loss |  1.63461924\n",
            "Step    430: eval          neg_log_perplexity | -1.63461924\n",
            "Step    430: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    430: Finished evaluation\n",
            "\n",
            "Step    440: Ran 10 train steps in 38.52 secs\n",
            "Step    440: Evaluation\n",
            "Step    440: train                   accuracy |  0.55829108\n",
            "Step    440: train                       loss |  1.60372639\n",
            "Step    440: train         neg_log_perplexity | -1.60372639\n",
            "Step    440: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    440: eval                    accuracy |  0.55466253\n",
            "Step    440: eval                        loss |  1.61750412\n",
            "Step    440: eval          neg_log_perplexity | -1.61750412\n",
            "Step    440: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    440: Finished evaluation\n",
            "\n",
            "Step    450: Ran 10 train steps in 38.67 secs\n",
            "Step    450: Evaluation\n",
            "Step    450: train                   accuracy |  0.56402808\n",
            "Step    450: train                       loss |  1.58513463\n",
            "Step    450: train         neg_log_perplexity | -1.58513463\n",
            "Step    450: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    450: eval                    accuracy |  0.56204385\n",
            "Step    450: eval                        loss |  1.59226477\n",
            "Step    450: eval          neg_log_perplexity | -1.59226477\n",
            "Step    450: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    450: Finished evaluation\n",
            "\n",
            "Step    460: Ran 10 train steps in 38.63 secs\n",
            "Step    460: Evaluation\n",
            "Step    460: train                   accuracy |  0.56893134\n",
            "Step    460: train                       loss |  1.56279111\n",
            "Step    460: train         neg_log_perplexity | -1.56279111\n",
            "Step    460: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    460: eval                    accuracy |  0.56776512\n",
            "Step    460: eval                        loss |  1.56660724\n",
            "Step    460: eval          neg_log_perplexity | -1.56660724\n",
            "Step    460: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    460: Finished evaluation\n",
            "\n",
            "Step    470: Ran 10 train steps in 38.38 secs\n",
            "Step    470: Evaluation\n",
            "Step    470: train                   accuracy |  0.57027137\n",
            "Step    470: train                       loss |  1.55413139\n",
            "Step    470: train         neg_log_perplexity | -1.55413139\n",
            "Step    470: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    470: eval                    accuracy |  0.57209986\n",
            "Step    470: eval                        loss |  1.54799724\n",
            "Step    470: eval          neg_log_perplexity | -1.54799724\n",
            "Step    470: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    470: Finished evaluation\n",
            "\n",
            "Step    480: Ran 10 train steps in 38.54 secs\n",
            "Step    480: Evaluation\n",
            "Step    480: train                   accuracy |  0.57569140\n",
            "Step    480: train                       loss |  1.53197026\n",
            "Step    480: train         neg_log_perplexity | -1.53197026\n",
            "Step    480: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    480: eval                    accuracy |  0.57523263\n",
            "Step    480: eval                        loss |  1.53380942\n",
            "Step    480: eval          neg_log_perplexity | -1.53380942\n",
            "Step    480: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    480: Finished evaluation\n",
            "\n",
            "Step    490: Ran 10 train steps in 38.44 secs\n",
            "Step    490: Evaluation\n",
            "Step    490: train                   accuracy |  0.57825410\n",
            "Step    490: train                       loss |  1.52264154\n",
            "Step    490: train         neg_log_perplexity | -1.52264154\n",
            "Step    490: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    490: eval                    accuracy |  0.57840300\n",
            "Step    490: eval                        loss |  1.52208531\n",
            "Step    490: eval          neg_log_perplexity | -1.52208531\n",
            "Step    490: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    490: Finished evaluation\n",
            "\n",
            "Step    500: Ran 10 train steps in 38.44 secs\n",
            "Step    500: Evaluation\n",
            "Step    500: train                   accuracy |  0.58246648\n",
            "Step    500: train                       loss |  1.50741208\n",
            "Step    500: train         neg_log_perplexity | -1.50741208\n",
            "Step    500: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    500: eval                    accuracy |  0.58169138\n",
            "Step    500: eval                        loss |  1.50931478\n",
            "Step    500: eval          neg_log_perplexity | -1.50931478\n",
            "Step    500: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    500: Finished evaluation\n",
            "\n",
            "Step    510: Ran 10 train steps in 38.35 secs\n",
            "Step    510: Evaluation\n",
            "Step    510: train                   accuracy |  0.58646083\n",
            "Step    510: train                       loss |  1.48988426\n",
            "Step    510: train         neg_log_perplexity | -1.48988426\n",
            "Step    510: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    510: eval                    accuracy |  0.58225095\n",
            "Step    510: eval                        loss |  1.50329709\n",
            "Step    510: eval          neg_log_perplexity | -1.50329709\n",
            "Step    510: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    510: Finished evaluation\n",
            "\n",
            "Step    520: Ran 10 train steps in 38.56 secs\n",
            "Step    520: Evaluation\n",
            "Step    520: train                   accuracy |  0.58883995\n",
            "Step    520: train                       loss |  1.48301303\n",
            "Step    520: train         neg_log_perplexity | -1.48301303\n",
            "Step    520: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    520: eval                    accuracy |  0.58737296\n",
            "Step    520: eval                        loss |  1.48737073\n",
            "Step    520: eval          neg_log_perplexity | -1.48737073\n",
            "Step    520: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    520: Finished evaluation\n",
            "\n",
            "Step    530: Ran 10 train steps in 38.47 secs\n",
            "Step    530: Evaluation\n",
            "Step    530: train                   accuracy |  0.58851171\n",
            "Step    530: train                       loss |  1.48133349\n",
            "Step    530: train         neg_log_perplexity | -1.48133349\n",
            "Step    530: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    530: eval                    accuracy |  0.58735740\n",
            "Step    530: eval                        loss |  1.48537171\n",
            "Step    530: eval          neg_log_perplexity | -1.48537171\n",
            "Step    530: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    530: Finished evaluation\n",
            "\n",
            "Step    540: Ran 10 train steps in 38.52 secs\n",
            "Step    540: Evaluation\n",
            "Step    540: train                   accuracy |  0.58999109\n",
            "Step    540: train                       loss |  1.47644174\n",
            "Step    540: train         neg_log_perplexity | -1.47644174\n",
            "Step    540: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    540: eval                    accuracy |  0.59054893\n",
            "Step    540: eval                        loss |  1.47456169\n",
            "Step    540: eval          neg_log_perplexity | -1.47456169\n",
            "Step    540: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    540: Finished evaluation\n",
            "\n",
            "Step    550: Ran 10 train steps in 38.49 secs\n",
            "Step    550: Evaluation\n",
            "Step    550: train                   accuracy |  0.59131384\n",
            "Step    550: train                       loss |  1.47065496\n",
            "Step    550: train         neg_log_perplexity | -1.47065496\n",
            "Step    550: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    550: eval                    accuracy |  0.59109926\n",
            "Step    550: eval                        loss |  1.47059429\n",
            "Step    550: eval          neg_log_perplexity | -1.47059429\n",
            "Step    550: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    550: Finished evaluation\n",
            "\n",
            "Step    560: Ran 10 train steps in 38.52 secs\n",
            "Step    560: Evaluation\n",
            "Step    560: train                   accuracy |  0.59340578\n",
            "Step    560: train                       loss |  1.46473229\n",
            "Step    560: train         neg_log_perplexity | -1.46473229\n",
            "Step    560: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    560: eval                    accuracy |  0.59082437\n",
            "Step    560: eval                        loss |  1.47331798\n",
            "Step    560: eval          neg_log_perplexity | -1.47331798\n",
            "Step    560: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    560: Finished evaluation\n",
            "\n",
            "Step    570: Ran 10 train steps in 38.45 secs\n",
            "Step    570: Evaluation\n",
            "Step    570: train                   accuracy |  0.59383929\n",
            "Step    570: train                       loss |  1.46233797\n",
            "Step    570: train         neg_log_perplexity | -1.46233797\n",
            "Step    570: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    570: eval                    accuracy |  0.59267032\n",
            "Step    570: eval                        loss |  1.46663094\n",
            "Step    570: eval          neg_log_perplexity | -1.46663094\n",
            "Step    570: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    570: Finished evaluation\n",
            "\n",
            "Step    580: Ran 10 train steps in 38.49 secs\n",
            "Step    580: Evaluation\n",
            "Step    580: train                   accuracy |  0.59400064\n",
            "Step    580: train                       loss |  1.46200633\n",
            "Step    580: train         neg_log_perplexity | -1.46200633\n",
            "Step    580: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    580: eval                    accuracy |  0.59220642\n",
            "Step    580: eval                        loss |  1.46704590\n",
            "Step    580: eval          neg_log_perplexity | -1.46704590\n",
            "Step    580: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    580: Finished evaluation\n",
            "\n",
            "Step    590: Ran 10 train steps in 38.53 secs\n",
            "Step    590: Evaluation\n",
            "Step    590: train                   accuracy |  0.59322596\n",
            "Step    590: train                       loss |  1.46409011\n",
            "Step    590: train         neg_log_perplexity | -1.46409011\n",
            "Step    590: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    590: eval                    accuracy |  0.59244990\n",
            "Step    590: eval                        loss |  1.46690929\n",
            "Step    590: eval          neg_log_perplexity | -1.46690929\n",
            "Step    590: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    590: Finished evaluation\n",
            "\n",
            "Step    600: Ran 10 train steps in 38.42 secs\n",
            "Step    600: Evaluation\n",
            "Step    600: train                   accuracy |  0.59294868\n",
            "Step    600: train                       loss |  1.46527922\n",
            "Step    600: train         neg_log_perplexity | -1.46527922\n",
            "Step    600: train weights_per_batch_per_core |  513812.00000000\n",
            "Step    600: eval                    accuracy |  0.59448308\n",
            "Step    600: eval                        loss |  1.46022844\n",
            "Step    600: eval          neg_log_perplexity | -1.46022844\n",
            "Step    600: eval  weights_per_batch_per_core |  513812.00000000\n",
            "Step    600: Finished evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY3hpgnI5Rgn",
        "colab_type": "text"
      },
      "source": [
        "## Sample from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffeLSbJk35pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As we report in the Reformer paper, increasing the number of hashing rounds\n",
        "# helps with quality. We can even increase the number of hashing rounds at\n",
        "# evaluation time only.\n",
        "gin.parse_config(\"\"\"LSHCausalAttention.n_hashes = 4\"\"\")\n",
        "model_infer = trax.models.ReformerLM(mode='predict')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "favRDt3U4CJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jit_model_infer = trax.layers.base._accelerate(model_infer._forward_internal, trax.math.device_count())\n",
        "infer_state = model_infer.new_weights_and_state(trax.supervised.trainer_lib.ShapeDtype((1,1), dtype=np.int32))[1]\n",
        "infer_state = trainer._for_n_devices(infer_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPIIk4dS4ULg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(length=2048, prompt=None):\n",
        "  model_weights = trainer._opt_state[0][0]\n",
        "\n",
        "  # Token id 0 is the equivalent of a \"start\" token\n",
        "  cur_inputs = np.zeros((trax.math.device_count(), 1, 1), dtype=np.int32)\n",
        "\n",
        "  cur_state = infer_state\n",
        "  rngs = trax.math.random.split(trax.math.random.get_prng(0), trax.math.device_count())\n",
        "  all_samples = []\n",
        "\n",
        "  if prompt is not None:\n",
        "    prompt = np.asarray(\n",
        "        [TOKENIZER.EncodeAsIds(prompt)] * trax.math.device_count())\n",
        "\n",
        "  for iteration in range(length):\n",
        "    logits, cur_state = jit_model_infer(\n",
        "        cur_inputs,\n",
        "        model_weights,\n",
        "        cur_state,\n",
        "        rngs)\n",
        "    \n",
        "    if prompt is not None and iteration < prompt.shape[1]:\n",
        "      cur_samples = onp.array(prompt[:, iteration], dtype=int)\n",
        "    else:\n",
        "      logits = onp.array(logits)[:,0,0,:]\n",
        "      probs = onp.exp(logits)\n",
        "      cur_samples = [onp.random.choice(probs.shape[-1], p=probs[i,:])\n",
        "                     for i in range(probs.shape[0])]\n",
        "      cur_samples = onp.array(cur_samples, dtype=int)\n",
        "    all_samples.append(cur_samples)\n",
        "\n",
        "    cur_inputs = np.array(cur_samples[:,None,None])\n",
        "  all_samples = onp.stack(all_samples, -1)\n",
        "  \n",
        "  return all_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUMTjX25HVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "74be1d77-67f1-4051-bcaf-fa667d17ce3d"
      },
      "source": [
        "samples = sample(length=128, prompt=\"There was a time when\")\n",
        "for ids in samples:\n",
        "  print(TOKENIZER.DecodeIds(ids.tolist()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There was a time when the door, when anxious--he did most of all kicking his weary. It was a scarcely realisease talking ears fellow stood next rough in extraords and then, a large stood old woman were died in the old woman, the accusing, and her little chest in handlinters,\n",
            "There was a time when came into desire any felt an injure of some of being a shopelessing, that, would certaints of fear where there is less in all true. In place would not copace of person that governoment, she is acquaintance. And yet talking office. What do you writ of some gament and\n",
            "There was a time when he lister, remained a little in his property in a day a man who in the room appearance of the rive of the subject was part of one acquaintied, even huge, and various comfined at things, that instantial ovelock-spons, eager girl had not looked feet\n",
            "There was a time when the balcasevsky Petrovitch who drown, scandlchedness of scanness, and forcertain rags, with coming an extremely colours and innatummed easier, and the absorbed in completely absorbed in completely forced with him at once. The red had been about\n",
            "There was a time when he remembered that there could believed. The suddenly over him. Inced to one clear eagger was a look of dish and no merely pictims of quisten. The dost visit, trivial about it. As the wooden people were companion of ceiling and a kitchen correctly forgetting her\n",
            "There was a time when he walked a stronger, however it was indeed from the noose inouple of the community, and it was pounced to be ashamed to be not!... Here... the address, Luished--no, the monst her! N-per circumstance of Golting it is myself for the room. But I am firmly w\n",
            "There was a time when he arrival in a raggong. At time he used to wall gave a girl like a lady who had come yard. Petrovitch pale and blank table in that behaved, turned in the eyebs, unno as iron window-drivering leaden in coming, that with a certicis. Not\n",
            "There was a time when he was ashes of flat. These were flound face was grate. As for a young man came into a room of notice of the room, a tortur jelooking, and with one acurg in the yard, with a minutely refinite an effect, and she laid which was walking with evidently agitation. E\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o31Wtxuu5Ehf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
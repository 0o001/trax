{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:20:49.409509Z",
     "start_time": "2021-10-21T19:20:49.407066Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.763601Z",
     "start_time": "2021-10-21T19:20:49.410970Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:06.758715Z",
     "iopub.status.busy": "2020-12-04T15:35:06.758464Z",
     "iopub.status.idle": "2020-12-04T15:35:37.278247Z",
     "shell.execute_reply": "2020-12-04T15:35:37.277568Z",
     "shell.execute_reply.started": "2020-12-04T15:35:06.758651Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import trax\n",
    "from absl import logging\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.778216Z",
     "start_time": "2021-10-21T19:21:15.765216Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLoop(training.Loop):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "        self._stop_training = False\n",
    "\n",
    "    def run(self, n_steps=1):\n",
    "        \"\"\"Just add a logic to break the loop to ``training.Loop.run`` when \n",
    "            the early stopping condition is satisfied. \n",
    "        \"\"\"\n",
    "        \n",
    "        with self._open_summary_writers() as (\n",
    "            train_summary_writers,\n",
    "            eval_summary_writers,\n",
    "        ):\n",
    "            process = psutil.Process(os.getpid())\n",
    "            loss_acc, step_acc = 0.0, 0\n",
    "            start_time = time.time()\n",
    "            optimizer_metrics_acc = collections.defaultdict(float)\n",
    "            for i in range(n_steps):\n",
    "                prev_task_index = self._which_task(self._step)\n",
    "                self._step += 1\n",
    "                task_index = self._which_task(self._step)\n",
    "                task_changed = task_index != prev_task_index\n",
    "\n",
    "                if task_changed:\n",
    "                    loss_acc, step_acc = 0.0, 0\n",
    "                    \n",
    "                loss, optimizer_metrics = self._run_one_step(task_index, task_changed)\n",
    "\n",
    "                optimizer_metrics, loss = fastmath.nested_map(\n",
    "                    functools.partial(tl.mean_or_pmean, self._n_devices),\n",
    "                    (optimizer_metrics, loss),\n",
    "                )\n",
    "\n",
    "                loss_acc += loss\n",
    "                # Log loss every 50 steps, every step in memory-efficient trainer.\n",
    "                if self._step % 50 == 0 or self._use_memory_efficient_trainer:\n",
    "                    self._log_step(\"Loss: %.4f\" % loss, stdout=False)\n",
    "                step_acc += 1\n",
    "                for metric_name, value in optimizer_metrics.items():\n",
    "                    optimizer_metrics_acc[metric_name] += value\n",
    "\n",
    "\n",
    "                if self._checkpoint_at(self.step):\n",
    "                    self.save_checkpoint(\"model\")\n",
    "                if self._permanent_checkpoint_at(self.step):\n",
    "                    self.save_checkpoint(f\"model_{self.step}\")\n",
    "                if self._eval_at(self.step):\n",
    "                    logging.info(\n",
    "                        \"cpu memory use (MB): %.2f\",\n",
    "                        process.memory_info().rss / float(1024 * 1024),\n",
    "                    )\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    self._log_training_progress(\n",
    "                        task=self._tasks[task_index],\n",
    "                        total_loss=loss_acc,\n",
    "                        n_steps=step_acc,\n",
    "                        elapsed_time=elapsed_time,\n",
    "                        optimizer_metrics=optimizer_metrics_acc,\n",
    "                        summary_writer=train_summary_writers[task_index],\n",
    "                    )\n",
    "                    self.run_evals(eval_summary_writers)\n",
    "                    loss_acc, step_acc = 0.0, 0\n",
    "                    start_time = time.time()\n",
    "                    optimizer_metrics_acc = collections.defaultdict(float)\n",
    "\n",
    "                if self._checkpoint_at(self.step):\n",
    "                    if self._checkpoint_low_metric is not None and self._at_lowest():\n",
    "                        self.save_checkpoint(f\"lowest_{self._checkpoint_low_metric}\")\n",
    "                    if self._checkpoint_high_metric is not None and self._at_highest():\n",
    "                        self.save_checkpoint(f\"highest_{self._checkpoint_high_metric}\")\n",
    "                \n",
    "                \n",
    "                for callback in self._callbacks:\n",
    "                    if callback.call_at(self.step):\n",
    "                        if callback.__class__.__name__ == 'EarlyStopping':\n",
    "                            #added to check for earlystopping callback after \n",
    "                            # history was updated.\n",
    "                            #callback.on_step_end execute before history was \n",
    "                            #updated. \n",
    "                            best_step = callback.on_step_begin_with_history(self.step)\n",
    "                            \n",
    "                            if not self._stop_training and self.step == n_steps:\n",
    "                                self._log_step(\"Did not meet early stopping condition.\")\n",
    "                        \n",
    "                \n",
    "                if self._stop_training:\n",
    "                    # added to stop the training.\n",
    "                    self._log_step(f\"Early stopping... \"\n",
    "                                  f\" the best step at {best_step}\")\n",
    "                    break\n",
    "                \n",
    "        self._eval_model.weights = self._model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.800616Z",
     "start_time": "2021-10-21T19:21:15.780224Z"
    }
   },
   "outputs": [],
   "source": [
    "def callback_earlystopper(\n",
    "    monitor=None, \n",
    "    min_delta=0, \n",
    "    patience=0, \n",
    "    mode=\"auto\", \n",
    "    restore_best_checkpoint=True\n",
    "):\n",
    "    \"\"\"Wrap the EarlyStopping class into a callable.\n",
    "\n",
    "    Returns an early stopping.\n",
    "\n",
    "    Args:\n",
    "    monitor: Quantity to be monitored.\n",
    "\n",
    "    min_delta: Minimum change in the monitored quantity\n",
    "        to qualify as an improvement, i.e. an absolute\n",
    "        change of less than min_delta, will count as no\n",
    "        improvement.\n",
    "\n",
    "    patience: ``patience`` times ``n_steps_per_checkpoint`` will be\n",
    "        the total number of steps without improvement\n",
    "        after which training will be stopped.\n",
    "\n",
    "    mode: One of ``{\"auto\", \"min\", \"max\"}``. In ``min``(``max``) mode,\n",
    "        training will stop when the quantity monitored has stopped\n",
    "        decreasing(increasing) during the number of steps assigned\n",
    "        in ``patience``; in ``\"auto\"``\n",
    "        mode, the direction is automatically inferred\n",
    "        from the name of the monitored quantity.\n",
    "\n",
    "    restore_best_checkpoint: Whether to restore model from\n",
    "        the checkpoint with the best value of the monitored quantity.\n",
    "        If False, the model weights obtained at the last step of\n",
    "        training are used. If True and there is an early stopping,\n",
    "        the best checkpoint will be restored.\n",
    "    \"\"\"\n",
    "\n",
    "    if mode not in [\"auto\", \"max\", \"min\"]:\n",
    "        self._loop._log_step(\n",
    "            f\"Early stopping mode='{mode}' is unknown, \" \"fallback to 'auto' mode\"\n",
    "        )\n",
    "        mode = \"auto\"\n",
    "\n",
    "    class EarlyStopping:\n",
    "        \"\"\"Create a call back taht activates early stopping.\n",
    "\n",
    "        Activate early stopping.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, loop):\n",
    "            \"\"\"Configures an early stopping.\n",
    "            This is inspired by keras.callbacks.EarlyStopping.\n",
    "\n",
    "            Args:\n",
    "                loop:   training ``Loop`` from the current training.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            self._loop = loop\n",
    "            self.monitor = monitor\n",
    "            self.min_delta = jnp.abs(min_delta)\n",
    "            self.patience = jnp.maximum(patience, 1)\n",
    "\n",
    "            self.restore_best_checkpoint = restore_best_checkpoint\n",
    "\n",
    "            if mode == \"min\":\n",
    "                self.monitor_op = jnp.less\n",
    "            elif mode == \"max\":\n",
    "                self.monitor_op = jnp.greater\n",
    "            else:\n",
    "                if self.monitor.endswith(\"Accuracy\"):\n",
    "                    self.monitor_op = jnp.greater\n",
    "                else:\n",
    "                    self.monitor_op = jnp.less\n",
    "\n",
    "            if self.monitor_op == np.greater:\n",
    "                self.min_delta *= 1\n",
    "            else:\n",
    "                self.min_delta *= -1\n",
    "\n",
    "            self.wait = 0\n",
    "            self.stopped_step = 1\n",
    "            self.best = jnp.inf if self.monitor_op == jnp.less else -jnp.inf\n",
    "            self.best_step = 1\n",
    "            self.best_checkpoint_path = None\n",
    "\n",
    "        def _is_metric_exist(self):\n",
    "            metric_names = [\n",
    "                name\n",
    "                for eval_task in self._loop._eval_tasks\n",
    "                for name in eval_task.metric_names\n",
    "            ]\n",
    "            return self.monitor in metric_names\n",
    "\n",
    "        def call_at(self, step):\n",
    "            return self._loop._eval_at(step)\n",
    "\n",
    "        def on_step_begin(self, step):\n",
    "            if not self._is_metric_exist():\n",
    "                # Raise error if the monitor name is not in evaluation task.\n",
    "                self._loop._log_step(\n",
    "                    f\"Early Stopping metric '{self.monitor}' \" \"is not in eval_tasks.\"\n",
    "                )\n",
    "                self._loop._log_step(\n",
    "                    \"Select one of \" f\"them from here {self.metric_names}.\"\n",
    "                )\n",
    "\n",
    "                raise SystemExit(\"Monitoring metric not found.\")\n",
    "\n",
    "        def on_step_end(self, step):\n",
    "            pass\n",
    "\n",
    "        def on_step_begin_with_history(self, step):\n",
    "            if self.restore_best_checkpoint and self.best_checkpoint_path is None:\n",
    "                self._loop.save_checkpoint(\"best_checkpoint\")\n",
    "                self.best_checkpoint_path = os.path.join(\n",
    "                    self._loop._output_dir, \"best_checkpoint.pkl.gz\"\n",
    "                )\n",
    "\n",
    "            self.wait += 1\n",
    "            current_step, current = self._get_monitor_value()\n",
    "\n",
    "            if current is None:\n",
    "                return\n",
    "\n",
    "            if self._is_improvement(current, self.best):\n",
    "                self.best = current\n",
    "                self.best_step = current_step\n",
    "                self._loop.save_checkpoint(\"best_checkpoint\")\n",
    "\n",
    "                # reset wait\n",
    "                self.wait = 0\n",
    "\n",
    "            if self.wait >= self.patience and step > 1:\n",
    "                self.stopped_step = current_step\n",
    "                self._loop._stop_training = True\n",
    "\n",
    "                if (\n",
    "                    self.restore_best_checkpoint\n",
    "                    and self.best_checkpoint_path is not None\n",
    "                ):\n",
    "                    self._loop.load_checkpoint(self.best_checkpoint_path)\n",
    "                    self._loop._log_step(\n",
    "                        f\"Best checkpoint was restored from Step {self.best_step}.\"\n",
    "                    )\n",
    "\n",
    "                return self.best_step\n",
    "\n",
    "        def _is_improvement(self, monitor_value, reference_value):\n",
    "            return self.monitor_op(monitor_value - self.min_delta, reference_value)\n",
    "\n",
    "        def _get_monitor_value(self):\n",
    "            step, monitor_value = self._loop.history.get(\n",
    "                \"eval\", \"metrics/\" + self.monitor\n",
    "            )[-1]\n",
    "            return step, monitor_value\n",
    "\n",
    "    return EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "## Generate data for linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.805691Z",
     "start_time": "2021-10-21T19:21:15.802488Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.279761Z",
     "iopub.status.busy": "2020-12-04T15:35:37.279529Z",
     "iopub.status.idle": "2020-12-04T15:35:37.283375Z",
     "shell.execute_reply": "2020-12-04T15:35:37.282592Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.279738Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_linear():\n",
    "    while True:\n",
    "        x=np.random.randint(low=1, high=10) * 1.0\n",
    "        y=x * 2.0 - 1\n",
    "        yield (np.array([x]), np.array([y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.811465Z",
     "start_time": "2021-10-21T19:21:15.807568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5.]), array([9.]))\n"
     ]
    }
   ],
   "source": [
    "data_linear = get_data_linear()\n",
    "print(next(data_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.815839Z",
     "start_time": "2021-10-21T19:21:15.813113Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.292101Z",
     "iopub.status.busy": "2020-12-04T15:35:37.291815Z",
     "iopub.status.idle": "2020-12-04T15:35:37.296048Z",
     "shell.execute_reply": "2020-12-04T15:35:37.295266Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.292054Z"
    }
   },
   "outputs": [],
   "source": [
    "data_pipeline = trax.data.Serial(trax.data.Batch(50), trax.data.AddLossWeights(),)\n",
    "data_stream = data_pipeline(data_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.821323Z",
     "start_time": "2021-10-21T19:21:15.817944Z"
    }
   },
   "outputs": [],
   "source": [
    "model_linear = tl.Serial(tl.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.849200Z",
     "start_time": "2021-10-21T19:21:15.822986Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.302605Z",
     "iopub.status.busy": "2020-12-04T15:35:37.302292Z",
     "iopub.status.idle": "2020-12-04T15:35:37.311629Z",
     "shell.execute_reply": "2020-12-04T15:35:37.311016Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.302575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Use the same data_stream for both training and evaluation\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=data_stream,\n",
    "    loss_layer=tl.L2Loss(),\n",
    "    optimizer=trax.optimizers.SGD(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=data_stream, metrics=[tl.L2Loss()], n_eval_batches=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add early stopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.857420Z",
     "start_time": "2021-10-21T19:21:15.854694Z"
    }
   },
   "outputs": [],
   "source": [
    "earlystopping=callback_earlystopper(monitor='L2Loss',min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:15.993079Z",
     "start_time": "2021-10-21T19:21:15.859169Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.313247Z",
     "iopub.status.busy": "2020-12-04T15:35:37.313032Z",
     "iopub.status.idle": "2020-12-04T15:35:37.442811Z",
     "shell.execute_reply": "2020-12-04T15:35:37.442187Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.313221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the training folder\n",
    "!rm -r linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:16.491382Z",
     "start_time": "2021-10-21T19:21:15.996076Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.444083Z",
     "iopub.status.busy": "2020-12-04T15:35:37.443918Z",
     "iopub.status.idle": "2020-12-04T15:35:39.043136Z",
     "shell.execute_reply": "2020-12-04T15:35:39.042484Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.444063Z"
    }
   },
   "outputs": [],
   "source": [
    "model_linear = tl.Serial(tl.Dense(1))\n",
    "training_loop = MyLoop(\n",
    "    model=model_linear, tasks=train_task, eval_tasks=[eval_task], output_dir=\"./linear_model\",\n",
    "    callbacks=[earlystopping]\n",
    ")\n",
    "# training_loop.save_checkpoint(f'step_{training_loop.step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:17.643821Z",
     "start_time": "2021-10-21T19:21:16.492560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2\n",
      "Step      1: Ran 1 train steps in 0.22 secs\n",
      "Step      1: train L2Loss |  229.90896606\n",
      "Step      1: eval  L2Loss |  46.22346115\n",
      "\n",
      "Step     10: Ran 9 train steps in 0.06 secs\n",
      "Step     10: train L2Loss |  6.04091883\n",
      "Step     10: eval  L2Loss |  0.42619219\n",
      "\n",
      "Step     20: Ran 10 train steps in 0.03 secs\n",
      "Step     20: train L2Loss |  0.42059857\n",
      "Step     20: eval  L2Loss |  0.36278656\n",
      "\n",
      "Step     30: Ran 10 train steps in 0.02 secs\n",
      "Step     30: train L2Loss |  0.34355703\n",
      "Step     30: eval  L2Loss |  0.35470238\n",
      "\n",
      "Step     40: Ran 10 train steps in 0.02 secs\n",
      "Step     40: train L2Loss |  0.31299758\n",
      "Step     40: eval  L2Loss |  0.30963793\n",
      "\n",
      "Step     50: Ran 10 train steps in 0.02 secs\n",
      "Step     50: train L2Loss |  0.31661826\n",
      "Step     50: eval  L2Loss |  0.30217770\n",
      "\n",
      "Step     60: Ran 10 train steps in 0.02 secs\n",
      "Step     60: train L2Loss |  0.30100703\n",
      "Step     60: eval  L2Loss |  0.27389389\n",
      "\n",
      "Step     70: Ran 10 train steps in 0.02 secs\n",
      "Step     70: train L2Loss |  0.24817976\n",
      "Step     70: eval  L2Loss |  0.26356390\n",
      "\n",
      "Step     80: Ran 10 train steps in 0.03 secs\n",
      "Step     80: train L2Loss |  0.23053572\n",
      "Step     80: eval  L2Loss |  0.24160929\n",
      "\n",
      "Step     90: Ran 10 train steps in 0.02 secs\n",
      "Step     90: train L2Loss |  0.22545198\n",
      "Step     90: eval  L2Loss |  0.21706782\n",
      "\n",
      "Step    100: Ran 10 train steps in 0.02 secs\n",
      "Step    100: train L2Loss |  0.20978431\n",
      "Step    100: eval  L2Loss |  0.19154793\n",
      "\n",
      "Step    110: Ran 10 train steps in 0.02 secs\n",
      "Step    110: train L2Loss |  0.19874811\n",
      "Step    110: eval  L2Loss |  0.17751113\n",
      "\n",
      "Step    120: Ran 10 train steps in 0.03 secs\n",
      "Step    120: train L2Loss |  0.17956892\n",
      "Step    120: eval  L2Loss |  0.16527717\n",
      "\n",
      "Step    130: Ran 10 train steps in 0.02 secs\n",
      "Step    130: train L2Loss |  0.16333854\n",
      "Step    130: eval  L2Loss |  0.16175196\n",
      "\n",
      "Step    140: Ran 10 train steps in 0.02 secs\n",
      "Step    140: train L2Loss |  0.15576053\n",
      "Step    140: eval  L2Loss |  0.14218976\n",
      "\n",
      "Step    150: Ran 10 train steps in 0.02 secs\n",
      "Step    150: train L2Loss |  0.14250457\n",
      "Step    150: eval  L2Loss |  0.11516149\n",
      "\n",
      "Step    160: Ran 10 train steps in 0.03 secs\n",
      "Step    160: train L2Loss |  0.12817430\n",
      "Step    160: eval  L2Loss |  0.11850417\n",
      "Step    160: Best checkpoint was restored from Step 150.\n",
      "Step    160: Early stopping...  the best step at 150\n"
     ]
    }
   ],
   "source": [
    "training_loop.run(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change patience \n",
    "patience = 10 means it will wait for 10 x 10 = 100 steps (patience * n_steps_per_checkpoint ) to before making a decision to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:17.648180Z",
     "start_time": "2021-10-21T19:21:17.645555Z"
    }
   },
   "outputs": [],
   "source": [
    "earlystopping=callback_earlystopper(monitor='L2Loss',patience=10,min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:17.781483Z",
     "start_time": "2021-10-21T19:21:17.650003Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.313247Z",
     "iopub.status.busy": "2020-12-04T15:35:37.313032Z",
     "iopub.status.idle": "2020-12-04T15:35:37.442811Z",
     "shell.execute_reply": "2020-12-04T15:35:37.442187Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.313221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the training folder\n",
    "!rm -r linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:17.814600Z",
     "start_time": "2021-10-21T19:21:17.783984Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.444083Z",
     "iopub.status.busy": "2020-12-04T15:35:37.443918Z",
     "iopub.status.idle": "2020-12-04T15:35:39.043136Z",
     "shell.execute_reply": "2020-12-04T15:35:39.042484Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.444063Z"
    }
   },
   "outputs": [],
   "source": [
    "model_linear = tl.Serial(tl.Dense(1))\n",
    "training_loop = MyLoop(\n",
    "    model=model_linear, tasks=train_task, eval_tasks=[eval_task], output_dir=\"./linear_model\",\n",
    "    callbacks=[earlystopping]\n",
    ")\n",
    "# training_loop.save_checkpoint(f'step_{training_loop.step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:23.010609Z",
     "start_time": "2021-10-21T19:21:17.816246Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2\n",
      "Step      1: Ran 1 train steps in 0.20 secs\n",
      "Step      1: train L2Loss |  81.69773102\n",
      "Step      1: eval  L2Loss |  11.24935627\n",
      "\n",
      "Step     10: Ran 9 train steps in 0.02 secs\n",
      "Step     10: train L2Loss |  1.63901091\n",
      "Step     10: eval  L2Loss |  0.30455348\n",
      "\n",
      "Step     20: Ran 10 train steps in 0.02 secs\n",
      "Step     20: train L2Loss |  0.28063789\n",
      "Step     20: eval  L2Loss |  0.26405337\n",
      "\n",
      "Step     30: Ran 10 train steps in 0.02 secs\n",
      "Step     30: train L2Loss |  0.28690392\n",
      "Step     30: eval  L2Loss |  0.25856414\n",
      "\n",
      "Step     40: Ran 10 train steps in 0.02 secs\n",
      "Step     40: train L2Loss |  0.25905538\n",
      "Step     40: eval  L2Loss |  0.21563198\n",
      "\n",
      "Step     50: Ran 10 train steps in 0.02 secs\n",
      "Step     50: train L2Loss |  0.22200949\n",
      "Step     50: eval  L2Loss |  0.21909043\n",
      "\n",
      "Step     60: Ran 10 train steps in 0.02 secs\n",
      "Step     60: train L2Loss |  0.19961575\n",
      "Step     60: eval  L2Loss |  0.18849312\n",
      "\n",
      "Step     70: Ran 10 train steps in 0.02 secs\n",
      "Step     70: train L2Loss |  0.18741669\n",
      "Step     70: eval  L2Loss |  0.19180961\n",
      "\n",
      "Step     80: Ran 10 train steps in 0.02 secs\n",
      "Step     80: train L2Loss |  0.17511705\n",
      "Step     80: eval  L2Loss |  0.16206446\n",
      "\n",
      "Step     90: Ran 10 train steps in 0.02 secs\n",
      "Step     90: train L2Loss |  0.17689568\n",
      "Step     90: eval  L2Loss |  0.15561798\n",
      "\n",
      "Step    100: Ran 10 train steps in 0.02 secs\n",
      "Step    100: train L2Loss |  0.15170833\n",
      "Step    100: eval  L2Loss |  0.13055749\n",
      "\n",
      "Step    110: Ran 10 train steps in 0.02 secs\n",
      "Step    110: train L2Loss |  0.13877818\n",
      "Step    110: eval  L2Loss |  0.13456459\n",
      "\n",
      "Step    120: Ran 10 train steps in 0.02 secs\n",
      "Step    120: train L2Loss |  0.13499276\n",
      "Step    120: eval  L2Loss |  0.12500881\n",
      "\n",
      "Step    130: Ran 10 train steps in 0.02 secs\n",
      "Step    130: train L2Loss |  0.11766177\n",
      "Step    130: eval  L2Loss |  0.11422719\n",
      "\n",
      "Step    140: Ran 10 train steps in 0.02 secs\n",
      "Step    140: train L2Loss |  0.10550620\n",
      "Step    140: eval  L2Loss |  0.10817631\n",
      "\n",
      "Step    150: Ran 10 train steps in 0.02 secs\n",
      "Step    150: train L2Loss |  0.09824397\n",
      "Step    150: eval  L2Loss |  0.10078197\n",
      "\n",
      "Step    160: Ran 10 train steps in 0.02 secs\n",
      "Step    160: train L2Loss |  0.09422737\n",
      "Step    160: eval  L2Loss |  0.09103017\n",
      "\n",
      "Step    170: Ran 10 train steps in 0.02 secs\n",
      "Step    170: train L2Loss |  0.08532009\n",
      "Step    170: eval  L2Loss |  0.08263365\n",
      "\n",
      "Step    180: Ran 10 train steps in 0.02 secs\n",
      "Step    180: train L2Loss |  0.08591609\n",
      "Step    180: eval  L2Loss |  0.07587243\n",
      "\n",
      "Step    190: Ran 10 train steps in 0.02 secs\n",
      "Step    190: train L2Loss |  0.07325494\n",
      "Step    190: eval  L2Loss |  0.06821842\n",
      "\n",
      "Step    200: Ran 10 train steps in 0.02 secs\n",
      "Step    200: train L2Loss |  0.06717323\n",
      "Step    200: eval  L2Loss |  0.06505584\n",
      "\n",
      "Step    210: Ran 10 train steps in 0.02 secs\n",
      "Step    210: train L2Loss |  0.06176293\n",
      "Step    210: eval  L2Loss |  0.05988232\n",
      "\n",
      "Step    220: Ran 10 train steps in 0.02 secs\n",
      "Step    220: train L2Loss |  0.05516310\n",
      "Step    220: eval  L2Loss |  0.05221633\n",
      "\n",
      "Step    230: Ran 10 train steps in 0.03 secs\n",
      "Step    230: train L2Loss |  0.05287005\n",
      "Step    230: eval  L2Loss |  0.05492221\n",
      "\n",
      "Step    240: Ran 10 train steps in 0.02 secs\n",
      "Step    240: train L2Loss |  0.04561957\n",
      "Step    240: eval  L2Loss |  0.04943951\n",
      "\n",
      "Step    250: Ran 10 train steps in 0.02 secs\n",
      "Step    250: train L2Loss |  0.04472163\n",
      "Step    250: eval  L2Loss |  0.04291537\n",
      "\n",
      "Step    260: Ran 10 train steps in 0.03 secs\n",
      "Step    260: train L2Loss |  0.04153470\n",
      "Step    260: eval  L2Loss |  0.03779905\n",
      "\n",
      "Step    270: Ran 10 train steps in 0.03 secs\n",
      "Step    270: train L2Loss |  0.03918057\n",
      "Step    270: eval  L2Loss |  0.03391368\n",
      "\n",
      "Step    280: Ran 10 train steps in 0.03 secs\n",
      "Step    280: train L2Loss |  0.03709646\n",
      "Step    280: eval  L2Loss |  0.03392628\n",
      "\n",
      "Step    290: Ran 10 train steps in 0.02 secs\n",
      "Step    290: train L2Loss |  0.03060167\n",
      "Step    290: eval  L2Loss |  0.02873613\n",
      "\n",
      "Step    300: Ran 10 train steps in 0.02 secs\n",
      "Step    300: train L2Loss |  0.02689723\n",
      "Step    300: eval  L2Loss |  0.02729172\n",
      "\n",
      "Step    310: Ran 10 train steps in 0.03 secs\n",
      "Step    310: train L2Loss |  0.02635413\n",
      "Step    310: eval  L2Loss |  0.02575011\n",
      "\n",
      "Step    320: Ran 10 train steps in 0.03 secs\n",
      "Step    320: train L2Loss |  0.02343236\n",
      "Step    320: eval  L2Loss |  0.02310533\n",
      "\n",
      "Step    330: Ran 10 train steps in 0.03 secs\n",
      "Step    330: train L2Loss |  0.02322184\n",
      "Step    330: eval  L2Loss |  0.02025560\n",
      "\n",
      "Step    340: Ran 10 train steps in 0.03 secs\n",
      "Step    340: train L2Loss |  0.02191620\n",
      "Step    340: eval  L2Loss |  0.02115710\n",
      "\n",
      "Step    350: Ran 10 train steps in 0.02 secs\n",
      "Step    350: train L2Loss |  0.01970291\n",
      "Step    350: eval  L2Loss |  0.01821795\n",
      "\n",
      "Step    360: Ran 10 train steps in 0.03 secs\n",
      "Step    360: train L2Loss |  0.01693000\n",
      "Step    360: eval  L2Loss |  0.01771631\n",
      "\n",
      "Step    370: Ran 10 train steps in 0.03 secs\n",
      "Step    370: train L2Loss |  0.01605746\n",
      "Step    370: eval  L2Loss |  0.01488345\n",
      "\n",
      "Step    380: Ran 10 train steps in 0.03 secs\n",
      "Step    380: train L2Loss |  0.01582001\n",
      "Step    380: eval  L2Loss |  0.01513121\n",
      "\n",
      "Step    390: Ran 10 train steps in 0.02 secs\n",
      "Step    390: train L2Loss |  0.01493330\n",
      "Step    390: eval  L2Loss |  0.01291117\n",
      "\n",
      "Step    400: Ran 10 train steps in 0.03 secs\n",
      "Step    400: train L2Loss |  0.01240410\n",
      "Step    400: eval  L2Loss |  0.01222494\n",
      "\n",
      "Step    410: Ran 10 train steps in 0.03 secs\n",
      "Step    410: train L2Loss |  0.01225612\n",
      "Step    410: eval  L2Loss |  0.01223606\n",
      "\n",
      "Step    420: Ran 10 train steps in 0.02 secs\n",
      "Step    420: train L2Loss |  0.01146764\n",
      "Step    420: eval  L2Loss |  0.01135569\n",
      "\n",
      "Step    430: Ran 10 train steps in 0.03 secs\n",
      "Step    430: train L2Loss |  0.00967694\n",
      "Step    430: eval  L2Loss |  0.00952428\n",
      "\n",
      "Step    440: Ran 10 train steps in 0.03 secs\n",
      "Step    440: train L2Loss |  0.00962396\n",
      "Step    440: eval  L2Loss |  0.00846390\n",
      "\n",
      "Step    450: Ran 10 train steps in 0.03 secs\n",
      "Step    450: train L2Loss |  0.00832805\n",
      "Step    450: eval  L2Loss |  0.00800904\n",
      "\n",
      "Step    460: Ran 10 train steps in 0.03 secs\n",
      "Step    460: train L2Loss |  0.00761518\n",
      "Step    460: eval  L2Loss |  0.00740900\n",
      "\n",
      "Step    470: Ran 10 train steps in 0.03 secs\n",
      "Step    470: train L2Loss |  0.00721762\n",
      "Step    470: eval  L2Loss |  0.00721005\n",
      "\n",
      "Step    480: Ran 10 train steps in 0.03 secs\n",
      "Step    480: train L2Loss |  0.00686910\n",
      "Step    480: eval  L2Loss |  0.00651221\n",
      "\n",
      "Step    490: Ran 10 train steps in 0.03 secs\n",
      "Step    490: train L2Loss |  0.00594670\n",
      "Step    490: eval  L2Loss |  0.00588538\n",
      "\n",
      "Step    500: Ran 10 train steps in 0.03 secs\n",
      "Step    500: train L2Loss |  0.00548987\n",
      "Step    500: eval  L2Loss |  0.00520434\n",
      "\n",
      "Step    510: Ran 10 train steps in 0.03 secs\n",
      "Step    510: train L2Loss |  0.00499567\n",
      "Step    510: eval  L2Loss |  0.00526077\n",
      "\n",
      "Step    520: Ran 10 train steps in 0.02 secs\n",
      "Step    520: train L2Loss |  0.00499217\n",
      "Step    520: eval  L2Loss |  0.00459826\n",
      "\n",
      "Step    530: Ran 10 train steps in 0.03 secs\n",
      "Step    530: train L2Loss |  0.00489318\n",
      "Step    530: eval  L2Loss |  0.00456792\n",
      "\n",
      "Step    540: Ran 10 train steps in 0.02 secs\n",
      "Step    540: train L2Loss |  0.00400250\n",
      "Step    540: eval  L2Loss |  0.00432536\n",
      "\n",
      "Step    550: Ran 10 train steps in 0.03 secs\n",
      "Step    550: train L2Loss |  0.00360364\n",
      "Step    550: eval  L2Loss |  0.00359848\n",
      "\n",
      "Step    560: Ran 10 train steps in 0.03 secs\n",
      "Step    560: train L2Loss |  0.00358307\n",
      "Step    560: eval  L2Loss |  0.00330881\n",
      "\n",
      "Step    570: Ran 10 train steps in 0.03 secs\n",
      "Step    570: train L2Loss |  0.00301846\n",
      "Step    570: eval  L2Loss |  0.00318250\n",
      "\n",
      "Step    580: Ran 10 train steps in 0.03 secs\n",
      "Step    580: train L2Loss |  0.00311808\n",
      "Step    580: eval  L2Loss |  0.00278917\n",
      "\n",
      "Step    590: Ran 10 train steps in 0.03 secs\n",
      "Step    590: train L2Loss |  0.00282914\n",
      "Step    590: eval  L2Loss |  0.00261720\n",
      "\n",
      "Step    600: Ran 10 train steps in 0.03 secs\n",
      "Step    600: train L2Loss |  0.00228848\n",
      "Step    600: eval  L2Loss |  0.00259075\n",
      "\n",
      "Step    610: Ran 10 train steps in 0.02 secs\n",
      "Step    610: train L2Loss |  0.00223942\n",
      "Step    610: eval  L2Loss |  0.00231108\n",
      "\n",
      "Step    620: Ran 10 train steps in 0.03 secs\n",
      "Step    620: train L2Loss |  0.00212007\n",
      "Step    620: eval  L2Loss |  0.00201980\n",
      "\n",
      "Step    630: Ran 10 train steps in 0.04 secs\n",
      "Step    630: train L2Loss |  0.00192588\n",
      "Step    630: eval  L2Loss |  0.00181323\n",
      "\n",
      "Step    640: Ran 10 train steps in 0.03 secs\n",
      "Step    640: train L2Loss |  0.00178990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    640: eval  L2Loss |  0.00182629\n",
      "\n",
      "Step    650: Ran 10 train steps in 0.02 secs\n",
      "Step    650: train L2Loss |  0.00179969\n",
      "Step    650: eval  L2Loss |  0.00168595\n",
      "\n",
      "Step    660: Ran 10 train steps in 0.04 secs\n",
      "Step    660: train L2Loss |  0.00168652\n",
      "Step    660: eval  L2Loss |  0.00158153\n",
      "\n",
      "Step    670: Ran 10 train steps in 0.04 secs\n",
      "Step    670: train L2Loss |  0.00143236\n",
      "Step    670: eval  L2Loss |  0.00141142\n",
      "\n",
      "Step    680: Ran 10 train steps in 0.04 secs\n",
      "Step    680: train L2Loss |  0.00137670\n",
      "Step    680: eval  L2Loss |  0.00120707\n",
      "\n",
      "Step    690: Ran 10 train steps in 0.05 secs\n",
      "Step    690: train L2Loss |  0.00125523\n",
      "Step    690: eval  L2Loss |  0.00111254\n",
      "\n",
      "Step    700: Ran 10 train steps in 0.03 secs\n",
      "Step    700: train L2Loss |  0.00107194\n",
      "Step    700: eval  L2Loss |  0.00101820\n",
      "\n",
      "Step    710: Ran 10 train steps in 0.04 secs\n",
      "Step    710: train L2Loss |  0.00102903\n",
      "Step    710: eval  L2Loss |  0.00095780\n",
      "\n",
      "Step    720: Ran 10 train steps in 0.03 secs\n",
      "Step    720: train L2Loss |  0.00095851\n",
      "Step    720: eval  L2Loss |  0.00092238\n",
      "\n",
      "Step    730: Ran 10 train steps in 0.03 secs\n",
      "Step    730: train L2Loss |  0.00091582\n",
      "Step    730: eval  L2Loss |  0.00082167\n",
      "\n",
      "Step    740: Ran 10 train steps in 0.05 secs\n",
      "Step    740: train L2Loss |  0.00075815\n",
      "Step    740: eval  L2Loss |  0.00071885\n",
      "\n",
      "Step    750: Ran 10 train steps in 0.05 secs\n",
      "Step    750: train L2Loss |  0.00075360\n",
      "Step    750: eval  L2Loss |  0.00070367\n",
      "\n",
      "Step    760: Ran 10 train steps in 0.03 secs\n",
      "Step    760: train L2Loss |  0.00067351\n",
      "Step    760: eval  L2Loss |  0.00065928\n",
      "\n",
      "Step    770: Ran 10 train steps in 0.03 secs\n",
      "Step    770: train L2Loss |  0.00062085\n",
      "Step    770: eval  L2Loss |  0.00057382\n",
      "\n",
      "Step    780: Ran 10 train steps in 0.04 secs\n",
      "Step    780: train L2Loss |  0.00057451\n",
      "Step    780: eval  L2Loss |  0.00052844\n",
      "\n",
      "Step    790: Ran 10 train steps in 0.03 secs\n",
      "Step    790: train L2Loss |  0.00056489\n",
      "Step    790: eval  L2Loss |  0.00051747\n",
      "\n",
      "Step    800: Ran 10 train steps in 0.03 secs\n",
      "Step    800: train L2Loss |  0.00050462\n",
      "Step    800: eval  L2Loss |  0.00043203\n",
      "\n",
      "Step    810: Ran 10 train steps in 0.04 secs\n",
      "Step    810: train L2Loss |  0.00045517\n",
      "Step    810: eval  L2Loss |  0.00041201\n",
      "\n",
      "Step    820: Ran 10 train steps in 0.03 secs\n",
      "Step    820: train L2Loss |  0.00040787\n",
      "Step    820: eval  L2Loss |  0.00037127\n",
      "\n",
      "Step    830: Ran 10 train steps in 0.03 secs\n",
      "Step    830: train L2Loss |  0.00034659\n",
      "Step    830: eval  L2Loss |  0.00033720\n",
      "\n",
      "Step    840: Ran 10 train steps in 0.03 secs\n",
      "Step    840: train L2Loss |  0.00034974\n",
      "Step    840: eval  L2Loss |  0.00032302\n",
      "\n",
      "Step    850: Ran 10 train steps in 0.04 secs\n",
      "Step    850: train L2Loss |  0.00031467\n",
      "Step    850: eval  L2Loss |  0.00029402\n",
      "\n",
      "Step    860: Ran 10 train steps in 0.03 secs\n",
      "Step    860: train L2Loss |  0.00030113\n",
      "Step    860: eval  L2Loss |  0.00027128\n",
      "\n",
      "Step    870: Ran 10 train steps in 0.03 secs\n",
      "Step    870: train L2Loss |  0.00025721\n",
      "Step    870: eval  L2Loss |  0.00025735\n",
      "\n",
      "Step    880: Ran 10 train steps in 0.03 secs\n",
      "Step    880: train L2Loss |  0.00025067\n",
      "Step    880: eval  L2Loss |  0.00023703\n",
      "\n",
      "Step    890: Ran 10 train steps in 0.03 secs\n",
      "Step    890: train L2Loss |  0.00023850\n",
      "Step    890: eval  L2Loss |  0.00021112\n",
      "\n",
      "Step    900: Ran 10 train steps in 0.04 secs\n",
      "Step    900: train L2Loss |  0.00020062\n",
      "Step    900: eval  L2Loss |  0.00018026\n",
      "\n",
      "Step    910: Ran 10 train steps in 0.03 secs\n",
      "Step    910: train L2Loss |  0.00019245\n",
      "Step    910: eval  L2Loss |  0.00017986\n",
      "\n",
      "Step    920: Ran 10 train steps in 0.03 secs\n",
      "Step    920: train L2Loss |  0.00017397\n",
      "Step    920: eval  L2Loss |  0.00017345\n",
      "\n",
      "Step    930: Ran 10 train steps in 0.03 secs\n",
      "Step    930: train L2Loss |  0.00015514\n",
      "Step    930: eval  L2Loss |  0.00015499\n",
      "\n",
      "Step    940: Ran 10 train steps in 0.03 secs\n",
      "Step    940: train L2Loss |  0.00014559\n",
      "Step    940: eval  L2Loss |  0.00013842\n",
      "\n",
      "Step    950: Ran 10 train steps in 0.03 secs\n",
      "Step    950: train L2Loss |  0.00014918\n",
      "Step    950: eval  L2Loss |  0.00013064\n",
      "\n",
      "Step    960: Ran 10 train steps in 0.03 secs\n",
      "Step    960: train L2Loss |  0.00013440\n",
      "Step    960: eval  L2Loss |  0.00012347\n",
      "\n",
      "Step    970: Ran 10 train steps in 0.03 secs\n",
      "Step    970: train L2Loss |  0.00011147\n",
      "Step    970: eval  L2Loss |  0.00011899\n",
      "\n",
      "Step    980: Ran 10 train steps in 0.03 secs\n",
      "Step    980: train L2Loss |  0.00009994\n",
      "Step    980: eval  L2Loss |  0.00010489\n",
      "\n",
      "Step    990: Ran 10 train steps in 0.05 secs\n",
      "Step    990: train L2Loss |  0.00010274\n",
      "Step    990: eval  L2Loss |  0.00008743\n",
      "\n",
      "Step   1000: Ran 10 train steps in 0.03 secs\n",
      "Step   1000: train L2Loss |  0.00008580\n",
      "Step   1000: eval  L2Loss |  0.00008543\n",
      "\n",
      "Step   1010: Ran 10 train steps in 0.03 secs\n",
      "Step   1010: train L2Loss |  0.00008140\n",
      "Step   1010: eval  L2Loss |  0.00008148\n",
      "\n",
      "Step   1020: Ran 10 train steps in 0.03 secs\n",
      "Step   1020: train L2Loss |  0.00007550\n",
      "Step   1020: eval  L2Loss |  0.00008384\n",
      "\n",
      "Step   1030: Ran 10 train steps in 0.03 secs\n",
      "Step   1030: train L2Loss |  0.00006508\n",
      "Step   1030: eval  L2Loss |  0.00006869\n",
      "\n",
      "Step   1040: Ran 10 train steps in 0.03 secs\n",
      "Step   1040: train L2Loss |  0.00006720\n",
      "Step   1040: eval  L2Loss |  0.00006600\n",
      "\n",
      "Step   1050: Ran 10 train steps in 0.03 secs\n",
      "Step   1050: train L2Loss |  0.00006604\n",
      "Step   1050: eval  L2Loss |  0.00006055\n",
      "\n",
      "Step   1060: Ran 10 train steps in 0.03 secs\n",
      "Step   1060: train L2Loss |  0.00005607\n",
      "Step   1060: eval  L2Loss |  0.00005541\n",
      "\n",
      "Step   1070: Ran 10 train steps in 0.03 secs\n",
      "Step   1070: train L2Loss |  0.00005400\n",
      "Step   1070: eval  L2Loss |  0.00004873\n",
      "\n",
      "Step   1080: Ran 10 train steps in 0.03 secs\n",
      "Step   1080: train L2Loss |  0.00005052\n",
      "Step   1080: eval  L2Loss |  0.00004636\n",
      "Step   1080: Best checkpoint was restored from Step 980.\n",
      "Step   1080: Early stopping...  the best step at 980\n"
     ]
    }
   ],
   "source": [
    "training_loop.run(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T19:21:23.047842Z",
     "start_time": "2021-10-21T19:21:23.012208Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-05T04:36:10.040691Z",
     "iopub.status.busy": "2020-12-05T04:36:10.040407Z",
     "iopub.status.idle": "2020-12-05T04:36:10.114322Z",
     "shell.execute_reply": "2020-12-05T04:36:10.113606Z",
     "shell.execute_reply.started": "2020-12-05T04:36:10.040657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 3.0100274],\n",
       "             [ 5.007715 ],\n",
       "             [18.991528 ],\n",
       "             [86.91291  ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=np.array([[2.0],[3.0],[10.0],[44.0]])\n",
    "model_linear(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trax",
   "language": "python",
   "name": "trax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

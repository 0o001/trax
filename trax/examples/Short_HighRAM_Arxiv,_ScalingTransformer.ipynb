{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAAzPCP8n05S"
      },
      "source": [
        "#@title\n",
        "# Copyright 2020 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzsxj2EV3lfL"
      },
      "source": [
        "# Scaling Transformers - Sparse Is Enough\n",
        "\n",
        "This colab contains all relevant code for the paper \"Sparse is Enough in Scaling Transformers\". We depend on th Trax library and the experiments in the paper were not run with the colab but in a distributed setup with the attached config files -- but with the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhbMDG8AYAvj"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AheIplg9BO9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax import fastmath\n",
        "from trax.fastmath import numpy as jnp\n",
        "from trax.supervised import training\n",
        "from trax.layers.assert_shape import assert_shape\n",
        "\n",
        "\n",
        "import copy\n",
        "import functools\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "from jax import test_util  # pylint: disable=unused-import\n",
        "from jax.config import config\n",
        "import numpy as np\n",
        "import psutil\n",
        "from tensorflow.compat.v2 import test\n",
        "\n",
        "from trax import fastmath\n",
        "from trax import layers as tl\n",
        "from trax import models\n",
        "from trax import shapes\n",
        "from trax.supervised import decoding\n",
        "import gin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H8G1C2sscY7"
      },
      "outputs": [],
      "source": [
        "from colabtools import adhoc_import\n",
        "import json\n",
        "import gc\n",
        "import jax\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import IPython.display as display\n",
        "import gin\n",
        "\n",
        "# Training colab using TRAX and sstables\n",
        "# The colab has three inspirations\n",
        "#   - https://colab.corp.le.com/drive/1J48CSaDjMcZSfJtKt_FecxPkdSjbD3up#scrollTo=przDFyGSqKwq\n",
        "#   - https://colab.corp..com/drive/1yXCCUDCNkJP1es5dwjhwnVOg1dWFjDT_#scrollTo=tRa53wxjKPT9\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "import functools\n",
        "\n",
        "from trax.data import tf_inputs\n",
        "import tensorflow_datasets as tfds\n",
        "from t5.data import preprocessors as t5_processors\n",
        "import t5.data\n",
        "\n",
        "from trax import data\n",
        "from trax import layers as tl\n",
        "from trax import models\n",
        "from trax import optimizers\n",
        "from trax.data import inputs\n",
        "from trax.supervised import lr_schedules\n",
        "from trax.supervised import trainer_lib\n",
        "from trax.rl import serialization_utils\n",
        "from trax.rl import space_serializer\n",
        "import math\n",
        "from trax.fastmath import numpy as numpy_math\n",
        "import trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiQR2Xhp4tHc"
      },
      "outputs": [],
      "source": [
        "from colabtools import adhoc_import\n",
        "  import trax..mira.mira_data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpt4I9_CX7dA"
      },
      "source": [
        "# Positional Encoding overriding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp-80B3gWE4U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from trax import fastmath\n",
        "from trax.fastmath import numpy as jnp\n",
        "from trax.layers import base\n",
        "from trax.layers import combinators as cb\n",
        "from trax.layers import core\n",
        "from trax.layers import initializers as init\n",
        "from trax.layers.assert_shape import assert_shape\n",
        "from trax.layers.base import Fn\n",
        "from trax.layers.research import sparsity\n",
        "\n",
        "@assert_shape('...d-\u003e...d')\n",
        "class PositionalEncoding(base.Layer):\n",
        "  \"\"\"Implements bare positional encoding.\n",
        "\n",
        "  Positional encoding includes a kind of dropout, if the layer is created in\n",
        "  ``'train'`` mode with a nonzero ``dropout`` value. For such a layer, on each\n",
        "  forward pass a subset of sequence positions selected at random will *not*\n",
        "  receive positional marking.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, max_len=2048, dropout=0.0, dropout_broadcast_dims=(-2,),\n",
        "               use_bfloat16=False, start_from_zero_prob=1.0,\n",
        "               max_offset_to_add=0, d_feature=64, mode='train'):\n",
        "    \"\"\"Creates a :py:class:`PositionalEncoding` instance in a given mode.\n",
        "\n",
        "    Args:\n",
        "      max_len: Maximum input sequence length.\n",
        "      dropout: Probability of *not* adding positional encoding to a sequence\n",
        "          position. Applies only if layer is created in ``'train'`` mode.\n",
        "      dropout_broadcast_dims: Axes along which dropout mask values are\n",
        "          broadcast rather than individually set at random.\n",
        "      use_bfloat16: If ``True``, use bfloat16 weights instead of the default\n",
        "        float32; this can save memory but may (rarely) lead to numerical issues.\n",
        "      start_from_zero_prob: how often to start from 0 during training,\n",
        "          (if 1.0, we always start from position 0, if less, we randomize).\n",
        "      max_offset_to_add: maximum offset to add to the positions during training\n",
        "        when randomizing; this offset plus input length must still be less than\n",
        "        max_len for all training examples.\n",
        "      d_feature: int or None; have this dimension for embeddings + shared FF if\n",
        "        not None.\n",
        "      mode: One of ``'train'``, ``'eval'``, or ``'predict'``.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self._max_len = max_len\n",
        "    if dropout \u003e= 1.0:\n",
        "      raise ValueError('Dropout rates must be lower than 1.')\n",
        "    if mode == 'train':\n",
        "      self._dropout = dropout\n",
        "    else:\n",
        "      self._dropout = 0.0\n",
        "    self._dropout_broadcast_dims = dropout_broadcast_dims\n",
        "    self._use_bfloat16 = use_bfloat16\n",
        "    self._start_from_zero_prob = start_from_zero_prob\n",
        "    self._max_offset_to_add = max_offset_to_add\n",
        "    self._mode = mode\n",
        "    self._d_feature = d_feature\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"Returns the input activations, with added positional information.\"\"\"\n",
        "    weights = self.weights\n",
        "    # if self._d_feature is not None and self._mode != 'predict':\n",
        "    if self._d_feature is not None:\n",
        "      weights, ff = weights\n",
        "      weights = jnp.dot(weights[:inputs.shape[1], :], ff)\n",
        "    if len(weights.shape) \u003c 3:  # old checkpoints have 1 in first dim already\n",
        "      weights = weights[None, :, :]  # [1, self._max_len, d_feature]\n",
        "    if self._mode != 'predict':\n",
        "      x = inputs\n",
        "      symbol_size = jnp.shape(x)[1]\n",
        "      if self._mode != 'train' or self._start_from_zero_prob \u003e= 1.0:\n",
        "        px = weights[:, :symbol_size, :]\n",
        "      else:\n",
        "        rng1, rng2 = fastmath.random.split(self.rng, 2)\n",
        "        start = fastmath.random.randint(rng1, (), 0, self._max_offset_to_add)\n",
        "        start_from_zero = fastmath.random.uniform(rng2, (), jnp.float32, 0, 1)\n",
        "        start = jnp.where(start_from_zero \u003c self._start_from_zero_prob,\n",
        "                          jnp.zeros((), dtype=jnp.int32), start)\n",
        "        px = fastmath.dynamic_slice_in_dim(weights, start, symbol_size,\n",
        "                                           axis=1)\n",
        "      if self._dropout == 0:\n",
        "        return x + px\n",
        "      else:\n",
        "        noise_shape = list(px.shape)\n",
        "        for dim in self._dropout_broadcast_dims:\n",
        "          noise_shape[dim] = 1\n",
        "        keep_prob = 1.0 - self._dropout\n",
        "        keep = fastmath.random.bernoulli(self.rng, keep_prob,\n",
        "                                         tuple(noise_shape))\n",
        "        multiplier = keep.astype(x.dtype) / keep_prob\n",
        "        return x + px * multiplier\n",
        "    else:\n",
        "      if self._dropout != 0:\n",
        "        raise ValueError(f'In predict mode, but dropout rate '\n",
        "                         f'({self._dropout}) is not zero.')\n",
        "\n",
        "      # State in this class is only used for fast inference. In that case,\n",
        "      # the model is called with consecutive elements position-by-position.\n",
        "      # This positional encoding layer stores the index of the current\n",
        "      # position and increments it on each call.\n",
        "      emb = fastmath.dynamic_slice_in_dim(\n",
        "          weights, self.state, inputs.shape[1], axis=1)\n",
        "      self.state += inputs.shape[1]\n",
        "      return inputs + emb\n",
        "\n",
        "  def init_weights_and_state(self, input_signature):\n",
        "    \"\"\"Randomly initializes the positional encoding vectors.\n",
        "\n",
        "    Args:\n",
        "      input_signature: :py:class:`ShapeDtype` instance characterizing the input\n",
        "          this layer should compute on.\n",
        "    \"\"\"\n",
        "    d_feature = input_signature.shape[-1]\n",
        "    if self._d_feature is not None:\n",
        "      d_feature = self._d_feature\n",
        "    pe = np.zeros((self._max_len, d_feature), dtype=np.float32)\n",
        "    position = np.arange(0, self._max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(\n",
        "        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)  # [self._max_len, d_feature]\n",
        "    if self._use_bfloat16:\n",
        "      pe = pe.astype(jnp.bfloat16)\n",
        "    w = jnp.array(pe)  # Trainable parameters, initialized above.\n",
        "    if self._d_feature is not None:\n",
        "      ff = init.GlorotUniformInitializer()(\n",
        "          (d_feature, input_signature.shape[-1]), self.rng)\n",
        "      self.weights = w, ff\n",
        "    else:\n",
        "      self.weights = w\n",
        "    if self._mode == 'predict':\n",
        "      self.state = jnp.zeros((), dtype=jnp.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be1pQgjEATPB"
      },
      "outputs": [],
      "source": [
        "og_PositionalEncoding = PositionalEncoding\n",
        "\n",
        "trax.layers.attention.PositionalEncoding = functools.partial(og_PositionalEncoding, d_feature=64)\n",
        "trax.layers.PositionalEncoding = functools.partial(og_PositionalEncoding, d_feature=64)\n",
        "tl.PositionalEncoding = functools.partial(og_PositionalEncoding, d_feature=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFAXUyFNX3kr"
      },
      "source": [
        "# Configurable Terraformer - copied implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t6Q21Q-2ygQ"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "from trax.models.reformer import reformer\n",
        "from trax.models.research import configurable_transformer as ct\n",
        "from trax.models.research import transformer2 as t2\n",
        "\n",
        "def ConfigurableTerraformer(input_vocab_size,\n",
        "                            output_vocab_size=None,\n",
        "                            d_model=512,\n",
        "                            d_ff=2048,\n",
        "                            d_attention_key=None,\n",
        "                            d_attention_value=None,\n",
        "                            n_encoder_layers=6,\n",
        "                            n_decoder_layers=6,\n",
        "                            n_heads=8,\n",
        "                            dropout=0.1,\n",
        "                            max_len=2048,\n",
        "                            encoder_attention_type=tl.SelfAttention,\n",
        "                            encoder_decoder_attention_type=tl.SelfAttention,\n",
        "                            pos_type='fixed-base',\n",
        "                            pos_axial_shape=(),\n",
        "                            pos_d_axial_embs=None,\n",
        "                            pos_start_from_zero_prob=1.0,\n",
        "                            pos_max_offset_to_add=0,\n",
        "                            ff_activation=tl.Relu,\n",
        "                            ff_use_sru=0,\n",
        "                            ff_chunk_size=0,\n",
        "                            ff_dropout=None,\n",
        "                            ff_sparsity=0,\n",
        "                            loss_sparsity_type='mult',\n",
        "                            loss_sparsity=0,\n",
        "                            loss_d_lowrank=0,\n",
        "                            loss_sparsity_prob=None,\n",
        "                            attention_chunk_size=0,\n",
        "                            n_layers_forget=0,\n",
        "                            forget_dense=True,\n",
        "                            n_decoder_attention_layers=2,\n",
        "                            use_bfloat16=False,\n",
        "                            reversible_encoder=False,\n",
        "                            use_two_swaps_per_encoder_block=True,\n",
        "                            center_layernorm=True,\n",
        "                            half_before_layer=None,\n",
        "                            double_after_layer=None,\n",
        "                            mode='train'):\n",
        "  \"\"\"Returns a highly configurable Terraformer encoder-decoder model.\n",
        "\n",
        "  This model maps paired text sequences (source and target) to float-valued\n",
        "  losses. If ``input_vocab_size`` is not ``None``, the layer takes\n",
        "  two input sequences:\n",
        "\n",
        "    - inputs (2):\n",
        "\n",
        "        - source: 2-D int array representing a batch of text strings via token\n",
        "          IDs plus padding markers; shape is `(batch_size, sequence_length)`,\n",
        "          where sequence_length \u003c= ``max_len``. Array elements are in\n",
        "          ``range(input_vocab_size)``, and 0 values mark padding positions.\n",
        "\n",
        "        - target: 2-D int array representing a batch of text strings via token\n",
        "          IDs plus padding markers; shape is `(batch_size, sequence_length)`,\n",
        "          where sequence_length \u003c= ``max_len``. Array elements are in\n",
        "          ``range(output_vocab_size)``, and 0 values mark padding positions.\n",
        "\n",
        "    - output: 1-D float array of losses; shape is `(batch_size)`.\n",
        "\n",
        "  If ``input_vocab_size`` is ``None``, the layer takes three input sequences:\n",
        "\n",
        "    - inputs (3):\n",
        "\n",
        "        - source: 3-D float array representing a batch of already-embedded text\n",
        "          strings; shape is `(batch_size, sequence_length, d_model)`, where\n",
        "          sequence_length \u003c= ``max_len``.\n",
        "\n",
        "        - mask: 2-D int array representing active versus masked positions; 0\n",
        "          values mark masked (padding) positions.\n",
        "\n",
        "        - target: 2-D int array representing a batch of text strings via token\n",
        "          IDs plus padding markers; shape is `(batch_size, sequence_length)`,\n",
        "          where sequence_length \u003c= ``max_len``. Array elements are in\n",
        "          ``range(output_vocab_size)``, and 0 values mark padding positions.\n",
        "\n",
        "    - output: 1-D float array of losses; shape is `(batch_size)`.\n",
        "\n",
        "  Args:\n",
        "    input_vocab_size: Input vocabulary size -- each element of the input tensor\n",
        "        should be an integer in ``range(vocab_size)``. These integers typically\n",
        "        represent token IDs from a vocabulary-based tokenizer.\n",
        "    output_vocab_size: If specified, gives the vocabulary size for the targets;\n",
        "        if ``None``, then input and target integers (token IDs) are assumed to\n",
        "        come from the same vocabulary.\n",
        "    d_model: Last/innermost dimension of activation arrays at most points in\n",
        "        the model, including the initial embedding output.\n",
        "    d_ff: Last/innermost dimension of special (typically wider)\n",
        "        :py:class:`Dense` layer in the feedforward part of each encoder block.\n",
        "    d_attention_key: Depth of key vectors in each attention head.\n",
        "    d_attention_value: Depth of value vectors in each attention head.\n",
        "    n_encoder_layers: Number of encoder blocks.\n",
        "    n_decoder_layers: Number of decoder blocks.\n",
        "    n_heads: Number of attention heads.\n",
        "    dropout: Stochastic rate (probability) for dropping an activation value\n",
        "        when applying dropout within encoder/decoder blocks. The same rate is\n",
        "        also used for attention dropout in encoder/decoder blocks.\n",
        "    max_len: Maximum symbol length for positional encoding.\n",
        "    encoder_attention_type: Type of attention to use in the encoder; must be\n",
        "        an attention-type subclass of :py:class:`trax.layers.Layer`.\n",
        "    encoder_decoder_attention_type: Type of attention to use in the decoder;\n",
        "        must be an attention-type subclass of :py:class:`trax.layers.Layer`.\n",
        "    pos_type: String indicating the type of positional embeddings to use.\n",
        "    pos_axial_shape: Shape (tuple of ints) to use for the axial position\n",
        "      encoding. If unset, axial position encoding is disabled.\n",
        "    pos_d_axial_embs: Tuple of ints specifying the depth of position embedding\n",
        "        for each axis. Tuple length must match ``pos_axial_shape``, and values\n",
        "        must sum to ``d_model``.\n",
        "    pos_start_from_zero_prob: Stochastic rate (probability) for starting\n",
        "        positional encoding at position 0 during training. If 1.0, always start\n",
        "        from position 0; if \u003c 1.0, the non-zero starts will be uniformly\n",
        "        distributed up to ``pos_max_offset_to_add``.\n",
        "    pos_max_offset_to_add: Maximum offset to add to positions during training\n",
        "        when randomizing. This offset plus input length must be less than\n",
        "        ``max_len`` for all training examples.\n",
        "    ff_activation: Type of activation function at the end of each block; must\n",
        "        be an activation-type subclass of :py:class:`trax.layers.Layer`.\n",
        "    ff_use_sru: If \u003e 0, use this number of SRU layers in place of feedforward\n",
        "        layers.\n",
        "    ff_chunk_size: If \u003e 0, chunk each feedforward layer into chunks of this\n",
        "        size.\n",
        "    ff_dropout: Stochastic rate (probability) for dropping an activation value\n",
        "        at feedforward nonlinearities.\n",
        "    ff_sparsity: If \u003e 0, use sparse feedforward blocks with this level of\n",
        "        sparsity.\n",
        "    loss_sparsity_type: String indicating the type of sparsity to used in loss\n",
        "        layer; see :py:class:`SparseDenseWithOptions` for options. If ``None``,\n",
        "        use no sparsity.\n",
        "    loss_sparsity: If \u003e 0, use this level of sparsity in the loss layer.\n",
        "    loss_d_lowrank: If \u003e 0, use a (low-rank) intermediate layer, with this\n",
        "        dimension, in the loss.\n",
        "    loss_sparsity_prob: Stochastic rate (probability) for using the sparse\n",
        "        version of the loss. If ``None``, use the sparse version exclusively.\n",
        "    attention_chunk_size: If \u003e 0, compute attention using chunks of this size.\n",
        "    n_layers_forget: How often to have a forgetting block between layers.\n",
        "    forget_dense: If True, use :py:class:`Dense` instances as forget layers;\n",
        "        else use no-ops.\n",
        "    n_decoder_attention_layers: Number of attention layers in a decoder block.\n",
        "    use_bfloat16: If True, use bfloat16 for weights; else use float32.\n",
        "    reversible_encoder: If True, make the encoder be reversible.\n",
        "    use_two_swaps_per_encoder_block: If True, ensure that there is a an even\n",
        "        number of swaps across the encoder.\n",
        "    center_layernorm: If True, use centering in :py:class:`LayerNorm` (the\n",
        "        default); else omit centering (which is known as RMS normalization).\n",
        "    half_before_layer: If not None, specifies an n'th layer such that all\n",
        "        layers before the n'th use half the normal values for ``d_model`` and\n",
        "        ``d_ff``.\n",
        "    double_after_layer: If not None, specifies an n'th layer such that all\n",
        "        layers after the n'th use double the normal values for ``d_model`` and\n",
        "        ``d_ff``.\n",
        "    mode: If ``'train'``, include dropout in each encoder/decoder block; else\n",
        "        dropout layers have no effect.\n",
        "\n",
        "  Returns:\n",
        "    A Terraformer encoder-decoder as a layer that maps from target and source\n",
        "    text sequences to a scalar loss.\n",
        "  \"\"\"\n",
        "  if mode == 'predict':\n",
        "    portal_mask = _PortalInput()\n",
        "  else:\n",
        "    portal_mask = None\n",
        "\n",
        "  # Set default dimensions for attention head key and value sizes.\n",
        "  if (d_model / 2) % n_heads != 0:\n",
        "    raise ValueError(f'n_heads ({n_heads}) must divide d_model/2 ({d_model/2})')\n",
        "  if d_attention_key is None:\n",
        "    d_attention_key = d_model // n_heads\n",
        "  if d_attention_value is None:\n",
        "    d_attention_value = d_model // n_heads\n",
        "\n",
        "  # Set values of d_model, d_ff and d_qkv for the first stage.\n",
        "  d_model1, d_ff1 = d_model, d_ff\n",
        "  d_attention_key1, d_attention_value1 = d_attention_key, d_attention_value\n",
        "  if half_before_layer:\n",
        "    d_model1, d_ff1 = d_model / 2, d_ff / 2\n",
        "    d_attention_key1 = d_attention_key / 2\n",
        "    d_attention_value1 = d_attention_value / 2\n",
        "\n",
        "  # Set values of d_model, d_ff and d_qkv for the final stage.\n",
        "  d_model2, d_ff2 = d_model, d_ff\n",
        "  d_attention_key2, d_attention_value2 = d_attention_key, d_attention_value\n",
        "  if double_after_layer:\n",
        "    d_model2, d_ff2 = d_model * 2, d_ff * 2\n",
        "    d_attention_key2 = d_attention_key * 2\n",
        "    d_attention_value2 = d_attention_value * 2\n",
        "\n",
        "  # Vector embeddings.\n",
        "  in_encoder, out_encoder, output_vocab_size = (\n",
        "      ct.EmbeddingAndPositionalEncodings(\n",
        "          input_vocab_size,\n",
        "          d_model1,\n",
        "          mode,\n",
        "          dropout,\n",
        "          [-2],  # dropout_shared_axes\n",
        "          max_len,\n",
        "          output_vocab_size=output_vocab_size,\n",
        "          pos_type=pos_type,\n",
        "          pos_axial_shape=pos_axial_shape,\n",
        "          pos_d_axial_embs=pos_d_axial_embs,\n",
        "          pos_start_from_zero_prob=pos_start_from_zero_prob,\n",
        "          pos_max_offset_to_add=pos_max_offset_to_add,\n",
        "          use_bfloat16=use_bfloat16)\n",
        "  )\n",
        "\n",
        "  def _EncoderBlock():\n",
        "    return reformer.EncoderBlock(\n",
        "        d_model1,\n",
        "        d_ff1,\n",
        "        n_heads,\n",
        "        encoder_attention_type,\n",
        "        dropout=dropout,\n",
        "        ff_activation=ff_activation,\n",
        "        ff_dropout=ff_dropout,\n",
        "        ff_use_sru=ff_use_sru,\n",
        "        ff_chunk_size=ff_chunk_size,\n",
        "        ff_sparsity=ff_sparsity,\n",
        "        attention_chunk_size=attention_chunk_size,\n",
        "        center_layernorm=center_layernorm,\n",
        "        use_bfloat16=use_bfloat16,\n",
        "        use_two_swaps_per_block=use_two_swaps_per_encoder_block,\n",
        "        mode=mode)\n",
        "\n",
        "  def _Encoder():  # vec_e mask_e tok_e tok_d tok_d\n",
        "    layers = [\n",
        "        tl.ReversibleSelect([0, 0]),\n",
        "        _ReversibleSerialForget(\n",
        "            [_EncoderBlock() for _ in range(n_encoder_layers)],\n",
        "            d_model1,\n",
        "            n_layers_forget,\n",
        "            forget_dense)\n",
        "    ]\n",
        "    if not reversible_encoder:\n",
        "      layers += [\n",
        "          _XYAvg(),\n",
        "          tl.Dense(d_model1, use_bfloat16=use_bfloat16),\n",
        "          tl.LayerNorm(),\n",
        "      ]\n",
        "    if mode == 'predict':\n",
        "      return tl.Cache(tl.Serial(layers))\n",
        "    else:\n",
        "      return tl.Serial(layers)\n",
        "\n",
        "  if mode == 'predict':\n",
        "    # TODO(jaszczur): Remove temporary fix of Terraformer padding in predict.\n",
        "    # In predict mode Terraformer needs masking for merged encoder-decoder\n",
        "    # sequence. This monkey patches the layer with a mask to neccessary places.\n",
        "    # This shouldn't be a permanent solution - mask should be passed through\n",
        "    # the stack and all the layers.\n",
        "    tl.attention.DotProductCausalAttention.monkey_patched_mask = (\n",
        "        lambda x: portal_mask)\n",
        "    tl.research.sparsity._RememberPad.monkey_patched_mask = (  # pylint: disable=protected-access\n",
        "        lambda x: portal_mask)\n",
        "    originalScanSRUCell = tl.rnn.ScanSRUCell\n",
        "    tl.rnn.ScanSRUCell = functools.partial(tl.rnn.ScanSRUCell,\n",
        "                                           monkey_patched_mask=portal_mask)\n",
        "\n",
        "  decoder_blocks = []\n",
        "\n",
        "  if isinstance(encoder_decoder_attention_type, (tuple, list)):\n",
        "    assert n_decoder_layers % len(encoder_decoder_attention_type) == 0\n",
        "  else:\n",
        "    encoder_decoder_attention_type = [encoder_decoder_attention_type]\n",
        "  for layer_idx in range(n_decoder_layers):\n",
        "    layer_attention_type = encoder_decoder_attention_type[\n",
        "        layer_idx % len(encoder_decoder_attention_type)]\n",
        "    # Grow d_model, d_ff, and d_qkv if requested.\n",
        "    d_m, d_f, d_k, d_v = d_model1, d_ff1, d_attention_key1, d_attention_value1\n",
        "    if half_before_layer and layer_idx \u003e= half_before_layer:\n",
        "      d_m, d_f, d_k, d_v = d_model, d_ff, d_attention_key, d_attention_value\n",
        "    if double_after_layer and layer_idx \u003e double_after_layer:\n",
        "      d_m, d_f, d_k, d_v = d_model2, d_ff2, d_attention_key2, d_attention_value2\n",
        "    decoder_block = reformer.DecoderBlock(\n",
        "        d_m, d_f, d_k, d_v, n_heads,\n",
        "        attention_type=layer_attention_type,\n",
        "        dropout=dropout,\n",
        "        ff_activation=ff_activation,\n",
        "        ff_dropout=ff_dropout,\n",
        "        ff_use_sru=ff_use_sru,\n",
        "        ff_chunk_size=ff_chunk_size,\n",
        "        ff_sparsity=ff_sparsity,\n",
        "        attention_chunk_size=attention_chunk_size,\n",
        "        n_attention_layers=n_decoder_attention_layers,\n",
        "        center_layernorm=center_layernorm,\n",
        "        use_bfloat16=use_bfloat16,\n",
        "        mode=mode)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "    if half_before_layer and layer_idx == half_before_layer - 1:\n",
        "      decoder_blocks.append(tl.ReversibleConcatenatePair())\n",
        "    if double_after_layer and layer_idx == double_after_layer:\n",
        "      decoder_blocks.append(tl.ReversibleConcatenatePair())\n",
        "\n",
        "  if mode == 'predict':\n",
        "    # After initializing the decoder we can revert to original state of\n",
        "    # previously monkey-patched classes/functions.\n",
        "    tl.attention.DotProductCausalAttention.monkey_patched_mask = (\n",
        "        lambda x: None)\n",
        "    tl.research.sparsity._RememberPad.monkey_patched_mask = (lambda x: None)  # pylint: disable=protected-access\n",
        "    tl.rnn.ScanSRUCell = originalScanSRUCell\n",
        "\n",
        "  def _Loss():\n",
        "    return tl.SparseDenseWithOptions(\n",
        "        output_vocab_size,\n",
        "        d_input=d_model2,\n",
        "        sparsity_type=loss_sparsity_type,\n",
        "        sparsity=loss_sparsity,\n",
        "        d_lowrank=loss_d_lowrank,\n",
        "        prob_sparse=loss_sparsity_prob,\n",
        "        use_bfloat16=use_bfloat16,\n",
        "        mode=mode)\n",
        "\n",
        "  def _enc_dec_concat():\n",
        "    \"\"\"Layers to merge encoder and decoder.\"\"\"\n",
        "    if reversible_encoder:\n",
        "      return [\n",
        "          tl.ReversibleSelect([0, 1, 4, 2, 3]),  # v_e v_d mask_e tok_e tok_d\n",
        "          t2.ConcatWithPadding2(mode=mode),      # v_ed v_ed tok_e tok_d\n",
        "      ]\n",
        "    else:\n",
        "      return [\n",
        "          tl.ReversibleSelect([0, 3, 1, 2]),     # v_e v_d mask_e tok_e tok_d\n",
        "          t2.ConcatWithPadding(mode=mode),       # v_ed tok_e tok_d\n",
        "          tl.ReversibleSelect([0, 0]),           # v_ed v_ed tok_e tok_d\n",
        "      ]\n",
        "\n",
        "  def _inp_layers():\n",
        "    if input_vocab_size is not None:\n",
        "\n",
        "      return tl.AssertFunction(\n",
        "          'bl,br-\u003ebld,bl,bl,br',  # b: batch, l/r: enc/dec length, d: vec depth\n",
        "        # return (\n",
        "          tl.Serial(  # tok_e tok_d\n",
        "              tl.Select([0, 0, 0, 1]),\n",
        "              tl.Parallel(in_encoder, [tl.PaddingMask(),\n",
        "                                       _RemoveAxes12()])\n",
        "          ))  # vec_e mask_e tok_e tok_d\n",
        "    else:\n",
        "      # Input in this case is vec_e, mask_e, tok_d. Where all downstream\n",
        "      # operations expect tok_e, we give it instead mask_e, expecting that\n",
        "      # downstream ops only are looking for padding/not padding.\n",
        "\n",
        "      return tl.AssertFunction(\n",
        "          'blf,bl,br-\u003ebld,bl,bl,br',  # f: in-feature depth, d: out-vector depth\n",
        "      # return (\n",
        "          tl.Serial(  # vec_e mask_e tok_d\n",
        "              tl.Select([0, 1, 1, 2]),\n",
        "              tl.Parallel(in_encoder, [], _AsTokenIDs())\n",
        "          ))  # vec_e mask_e tok_e tok_d\n",
        "\n",
        "  # Assemble and return the model.\n",
        "  return tl.Serial(\n",
        "      _inp_layers(),               # vec_e mask_e tok_e tok_d\n",
        "      tl.Parallel([], portal_mask),\n",
        "\n",
        "      tl.Select([0, 1, 2, 3, 3]),  # Copy decoder tokens for use in loss.\n",
        "\n",
        "      # Embed in and out tokens; done together as weights may be shared.\n",
        "      tl.Parallel([], [], [], [tl.ShiftRight(mode=mode),\n",
        "                               out_encoder]),  # vec_e mask_e tok_e vec_d tok_d\n",
        "\n",
        "      # Encode; then concat encoder and decoder, given encoder mask.\n",
        "      _Encoder(),                             # vec_e mask_e tok_e vec_d tok_d\n",
        "      _enc_dec_concat(),\n",
        "\n",
        "      # Run decoder blocks.\n",
        "      _ReversibleSerialForget(decoder_blocks, d_model2, n_layers_forget,\n",
        "                              forget_dense),  # vec_ed1 vec_ed2 tok_e tok_d\n",
        "      _XYAvg(),                               # vec_ed tok_e tok_d\n",
        "      tl.LayerNorm(),                         # vec_ed tok_e tok_d\n",
        "\n",
        "      # Separate out the encoder part from the concatenated vector,\n",
        "      # then compute loss.\n",
        "      tl.Select([0, 1, 2, 2]),                        # vec_ed tok_e tok_d tok_d\n",
        "      t2.StripFromConcatenateWithPadding(mode=mode),  # vec_d tok_d\n",
        "      _Loss(),  # vec_d tok_d\n",
        "  )\n",
        "\n",
        "\n",
        "def _InsertAxes12():\n",
        "  \"\"\"Returns a layer that inserts two internal size-1 axes into an array.\"\"\"\n",
        "  return tl.Fn('InsertAxes12',\n",
        "               lambda x: jnp.reshape(x, (x.shape[0], 1, 1, x.shape[1])))\n",
        "\n",
        "\n",
        "def _RemoveAxes12():\n",
        "  \"\"\"Returns a layer that removes two internal size-1 axes from an array.\"\"\"\n",
        "  return tl.Fn('RemoveAxes12', lambda x: jnp.squeeze(x, (1, 2)))\n",
        "\n",
        "\n",
        "def _AsTokenIDs():\n",
        "  \"\"\"Returns a layer that makes mask values look like token ID ints.\"\"\"\n",
        "  return tl.Fn('AsTokenIDs', lambda x: x.astype(jnp.int32))\n",
        "\n",
        "\n",
        "def _XYAvg():\n",
        "  \"\"\"Returns a layer that computes the element-wise average of two arrays.\"\"\"\n",
        "  return tl.Fn('XYAvg', lambda x, y: (x + y) / 2.0)\n",
        "\n",
        "\n",
        "def _ReversibleSerialForget(layers, d_model, n_layers, forget_dense=True):\n",
        "  \"\"\"ReversibleSerial but with a forgetting block every n_layers.\"\"\"\n",
        "  if not n_layers or len(layers) \u003c= n_layers + 1:\n",
        "    return tl.ReversibleSerial(layers)\n",
        "  layers1, layers2 = layers[:n_layers], layers[n_layers:]\n",
        "\n",
        "  if forget_dense:\n",
        "    forgetting_layer = tl.Serial(\n",
        "        _XYAvg(),\n",
        "        tl.Dense(d_model),\n",
        "        tl.Dup(),\n",
        "    )\n",
        "  else:\n",
        "    forgetting_layer = tl.Select([0, 1])\n",
        "\n",
        "  return tl.Serial(\n",
        "      tl.ReversibleSerial(layers1),\n",
        "      forgetting_layer,\n",
        "      _ReversibleSerialForget(layers2, d_model, n_layers, forget_dense)\n",
        "  )\n",
        "\n",
        "\n",
        "def _ConvertToNaNsOnAnyZero():\n",
        "  def _convert_to_nans(x, y):\n",
        "    # if all values in y are non-zeros, return x; otherwise return 0s\n",
        "    return jnp.where(jnp.all(y, keepdims=False), x, x/0.), y\n",
        "  return tl.Fn('ConvertToNaNsOnAnyZero', _convert_to_nans, n_out=2)\n",
        "\n",
        "\n",
        "class _PortalInput(tl.Layer):\n",
        "  \"\"\"Portal input for monkey-patching of mask in predict mode.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__(name='_PortalInput', n_out=1, n_in=1)\n",
        "    self._portal_output = _PortalOutput(self)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if isinstance(x, (list, tuple)):\n",
        "      x = x[0]\n",
        "    self.state = (x,)\n",
        "    return x\n",
        "\n",
        "  def init_weights_and_state(self, input_signature):\n",
        "    \"\"\"Initializes this layer's weights.\"\"\"\n",
        "    if isinstance(input_signature, (list, tuple)):\n",
        "      input_signature = input_signature[0]\n",
        "    self.state = (jnp.zeros(input_signature.shape),)\n",
        "\n",
        "  def get_value(self):\n",
        "    return self.state[0]\n",
        "\n",
        "  def get_layer(self):\n",
        "    return self._portal_output\n",
        "\n",
        "\n",
        "class _PortalOutput(tl.Layer):\n",
        "  \"\"\"Portal input for monkey-patching of mask in predict mode.\"\"\"\n",
        "\n",
        "  def __init__(self, portal_input):\n",
        "    super().__init__(name='_PortalOutput', n_out=1, n_in=0)\n",
        "    self._portal_input = portal_input\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self._portal_input.get_value()\n",
        "\n",
        "  def get_value(self):\n",
        "    return self._portal_input.get_value()\n",
        "\n",
        "\n",
        "import gin\n",
        "gin.enter_interactive_mode()\n",
        "def model_configure(*args, **kwargs):\n",
        "  kwargs['module'] = 'trax.models'\n",
        "  return gin.external_configurable(*args, **kwargs)\n",
        "\n",
        "trax.models.reformer.ConfigurableTerraformer = ConfigurableTerraformer\n",
        "\n",
        "trax.models.ConfigurableTerraformer = model_configure(trax.models.reformer.ConfigurableTerraformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkgeIq5GYGuv"
      },
      "source": [
        "# copying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Jfo7vwp_X6"
      },
      "outputs": [],
      "source": [
        "gs_link = \"gs://trax-ml/terraformer\"\n",
        "mira_xm2a = \"https://xm2a.corp..com/experiments/25250921\"\n",
        "_mira_xm2a_main = \"//gc-d/home/afrozm/rs=6.3/mira_med-05-11-07-41/model_200000.pkl.gz\"\n",
        "_mira_xm2a_weights = \"//gc-d/home/afrozm/rs=6.3/mira_med-05-11-07-41/model_200000.weights.npy.gz\"\n",
        "_mira_xm2a_opt_slots = \"//gc-d/home/afrozm/rs=6.3/mira_med-05-11-07-41/model_200000.opt_slots0.npy.gz\"\n",
        "_mira_xm2a_config = \"//gc-d/home/afrozm/rs=6.3/mira_med-05-11-07-41/config.gin\"\n",
        "\n",
        "mira_xm2a_big = \"https://xm2a.corp..com/experiments/24886122\"\n",
        "_mira_xm2a_big_main = \"//gc-d/home/afrozm/rs=6.3/mira_big-04-02-06-20/model_210000.pkl.gz\"\n",
        "_mira_xm2a_big_weights = \"//gc-d/home/afrozm/rs=6.3/mira_big-04-02-06-20/model_210000.weights.npy.gz\"\n",
        "_mira_xm2a_big_opt_slots = \"//gc-d/home/afrozm/rs=6.3/mira_big-04-02-06-20/model_210000.opt_slots0.npy.gz\"\n",
        "_mira_xm2a_big_config = \"//gc-d/home/afrozm/rs=6.3/mira_big-04-02-06-20/config.gin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoy0lyC7LT_s"
      },
      "outputs": [],
      "source": [
        "gin_file = _mira_xm2a_config\n",
        "files = !ls /tmp\n",
        "\n",
        "if 'model_med' not in files:\n",
        "  ! cp {gin_file} /tmp\n",
        "  !mkdir /tmp/model_med\n",
        "  ! cp -f //gc-d/home/afrozm/rs=6.3/mira_med-05-11-07-41/model_200000* /tmp/model_med\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLMsx1RPXoh2"
      },
      "source": [
        "# parsing gin config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G96iYETmw6F"
      },
      "outputs": [],
      "source": [
        "f = open('/tmp/config.gin')\n",
        "gin_config = list(f)\n",
        "f.close()\n",
        "#\n",
        "#  Uncomment this part to get the original results from the docs\n",
        "#\n",
        "# keep_gin = [l for l in gin_config if 'predict_mem' not in l]\n",
        "# change_gin = [l for l in gin_config if 'predict_mem' in l]\n",
        "# changed_gin = [l[:-6] + '2048\\n' for l in change_gin]\n",
        "# gin_config = keep_gin + changed_gin\n",
        "# keep_gin = [l for l in gin_config if 'predict_drop' not in l]\n",
        "# change_gin = [l for l in gin_config if 'predict_drop' in l]\n",
        "# changed_gin = [l[:-6] + '2048\\n' for l in change_gin]\n",
        "# gin_config = keep_gin + changed_gin\n",
        "\n",
        "\n",
        "# keep_gin = [l for l in gin_config if 'std_length' not in l]\n",
        "# change_gin = [l for l in gin_config if 'std_length' in l]\n",
        "# changed_gin = [l[:-5] + '2048\\n' for l in change_gin]\n",
        "# gin_config = keep_gin + changed_gin\n",
        "\n",
        "\n",
        "# keep_gin = [l for l in gin_config if 'max_length = 512' not in l]\n",
        "# change_gin = [l for l in gin_config if 'max_length = 512' in l]\n",
        "# changed_gin = ['max_length = 2048\\n' for l in change_gin]\n",
        "# gin_config = keep_gin + changed_gin\n",
        "#\n",
        "#  End of the part that needs to be uncommented.\n",
        "#\n",
        "\n",
        "gin_config = [l.replace('Reformer2', 'ConfigurableTerraformer') for l in gin_config]\n",
        "\n",
        "gin_config.append(\n",
        "    'DotProductCausalAttention.max_inference_length = 2048'\n",
        ")\n",
        "\n",
        "og_DotProductCausalAttention = trax.layers.attention.DotProductCausalAttention\n",
        "trax.layers.attention.DotProductCausalAttention = functools.partial(\n",
        "    og_DotProductCausalAttention, max_inference_length=2048,\n",
        ")\n",
        "\n",
        "# gin_config.append(\n",
        "#     'MixedLSHSelfAttention.std_length='\n",
        "# )\n",
        "\n",
        "gin_config = ''.join(gin_config)\n",
        "gin.parse_config(gin_config)\n",
        "gin.operative_config_str().split('\\n')\n",
        "\n",
        "print(gin_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO4m1irBKv9z"
      },
      "outputs": [],
      "source": [
        "# encoder/MixedLSHSelfAttention.predict_drop_len = 32768\n",
        "# encoder/MixedLSHSelfAttention.predict_mem_len = 32768\n",
        "# encoder/MixedLSHSelfAttention.std_length = 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckHGiDVvXrgl"
      },
      "source": [
        "# random stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1942b7_r7T70"
      },
      "outputs": [],
      "source": [
        "def model(mode):\n",
        "  return models.ConfigurableTerraformer(mode=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHO398cVFfiw"
      },
      "outputs": [],
      "source": [
        "padding_fun = trax.data.PadToLength(len_map={0: 512, 1: 512, 2: 512}, pad_value = {0: 0, 1: 0, 2:0})\n",
        "question = \"\"\"code:\n",
        "def square_list(xs):\n",
        "  return [\u003cSENTINEL\u003e for x in xs]\n",
        "print(square_list([1, 2, 3, 4]))\n",
        "\n",
        "output:\n",
        "[1, 4, 9, 16]\"\"\"\n",
        "\n",
        "tokenized = next(padding_fun(trax.data.tokenize([question,], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)))\n",
        "print(trax.data.detokenize(tokenized, vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100))\n",
        "print(tokenized.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6BGwveWXdwR"
      },
      "source": [
        "# autoregressive_sample_stream etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhfwHP2daQIc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "def autoregressive_sample_stream(model, inputs=None,\n",
        "                                 batch_size=1, temperature=1.0,\n",
        "                                 start_id=2, accelerate=True, prefix=None):\n",
        "  if inputs is not None and inputs.shape[0] != batch_size:\n",
        "    raise ValueError(f'Inputs batch size ({inputs.shape[0]}) does not match '\n",
        "                     f'batch_size arg ({batch_size}.')\n",
        "\n",
        "  fast_model = tl.Accelerate(model) if accelerate else model\n",
        "  if np.isscalar(start_id):\n",
        "    start_symbol = np.full((batch_size, 1), start_id, dtype=np.int32)\n",
        "  else:\n",
        "    start_symbol = start_id\n",
        "  if model.n_in == 1 and inputs is not None:\n",
        "    current_symbols = np.concatenate([start_symbol, inputs], axis=1)\n",
        "  else:\n",
        "    if prefix is None:\n",
        "      current_symbols = start_symbol\n",
        "    else:\n",
        "      current_symbols = np.concatenate([start_symbol, prefix], axis=1)\n",
        "\n",
        "  while True:\n",
        "    t0 = time.time()\n",
        "    if model.n_in \u003e 1 and inputs is not None:\n",
        "      # print(\"inp, curr:\", inputs.shape, current_symbols.shape)\n",
        "      logits = fast_model((inputs, current_symbols))[0]\n",
        "    else:\n",
        "      logits = fast_model(current_symbols)\n",
        "    # print('logits:', str(logits)[:100])\n",
        "    logits = tl.log_softmax(logits[:, -1, :])\n",
        "    sample = tl.logsoftmax_sample(logits, temperature=temperature)\n",
        "\n",
        "    print(trax.data.detokenize(sample, vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100))\n",
        "    print(\"Time per token: {}\".format(time.time() - t0))\n",
        "    sys.stdout.flush();\n",
        "    \n",
        "    yield sample\n",
        "    # NOTE: Because the model is autoregressive and in 'predict' mode, its\n",
        "    # history is cached in the model state and the next input is the single\n",
        "    # symbol just sampled.\n",
        "    current_symbols = sample[:, None]\n",
        "\n",
        "\n",
        "START_INDEX = 10\n",
        "\n",
        "def autoregressive_sample(model, inputs=None,\n",
        "                          batch_size=1, temperature=1.0,\n",
        "                          start_id=0, eos_id=1, max_length=100,\n",
        "                          accelerate=True, prefix=None):\n",
        "  result = []\n",
        "  eos_seen = []\n",
        "  counter = 0\n",
        "  for index, sample in enumerate(autoregressive_sample_stream(\n",
        "      model, inputs, batch_size=batch_size, temperature=temperature,\n",
        "      start_id=start_id, accelerate=accelerate, prefix=prefix)):\n",
        "    if index == START_INDEX:\n",
        "      start_time = time.time()\n",
        "    sample = sample[:, None]\n",
        "    result.append(sample)\n",
        "    counter += 1\n",
        "    if counter \u003e= max_length:\n",
        "      print('decoded one token per {} s'.format(\n",
        "          (time.time()-start_time)/(index-START_INDEX)))\n",
        "      return np.concatenate(result, axis=1)\n",
        "    # Check at which batch positions have we already encountered EOS.\n",
        "    for j in range(batch_size):\n",
        "      if int(sample[j, 0]) == eos_id:\n",
        "        eos_seen.append(j)\n",
        "    # If EOS has been seen on all positions, stop.\n",
        "    if all([j in eos_seen for j in range(batch_size)]):\n",
        "      print('decoded one token per {} s'.format(\n",
        "          (time.time()-start_time)/(index-START_INDEX)))\n",
        "      return np.concatenate(result, axis=1)\n",
        "  print('decoded one token per {} s'.format(\n",
        "      (time.time()-start_time)/(index-START_INDEX)))\n",
        "  return np.concatenate(result, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZq2dZRtXiDi"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q56arz7ioAJ6"
      },
      "outputs": [],
      "source": [
        "model_file = \"/tmp/model_med/model_200000.pkl.gz\"\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=numpy_math.int32)\n",
        "# The model does not like other numbers than 1024 in the line below.\n",
        "# In particular 15 * 1024 does not work.\n",
        "shape1l = trax.shapes.ShapeDtype((1, 1024), dtype=numpy_math.int32)\n",
        "\n",
        "with trax.fastmath.use_backend(trax.fastmath.Backend.JAX):\n",
        "  model_predict = model(mode='predict')\n",
        "\n",
        "  \n",
        "  model_predict.init_from_file(model_file, weights_only=True,\n",
        "                              input_signature=(shape1l, shape11))\n",
        "  old_state = model_predict.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2iT1HQDbe1k"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "dataset = tfds.summarization.scientific_papers.ScientificPapers()\n",
        "valid = tfds.load(name='scientific_papers/arxiv:1.1.1')['test']\n",
        "index = 0\n",
        "xarts = []\n",
        "for x in valid:\n",
        "  xarts.append(x)\n",
        "  index += 1\n",
        "  if index == 3:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzN0oOa0CxyS"
      },
      "outputs": [],
      "source": [
        "# Decode the first article\n",
        "xart = xarts[0]['article']\n",
        "question = xart.numpy().decode()\n",
        "print(question[:512])\n",
        "\n",
        "tokenized = next(padding_fun(trax.data.tokenize([question,], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)))\n",
        "trax.data.detokenize(tokenized, vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)\n",
        "\n",
        "with trax.fastmath.use_backend(trax.fastmath.Backend.JAX):\n",
        "  model_predict.state = old_state\n",
        "  \n",
        "  # Putting below 15*1024 does not work. \n",
        "  tokens = autoregressive_sample(model_predict, tokenized[None,:1024], temperature=0., max_length=50)\n",
        "  print(tokens) \n",
        "  print(trax.data.detokenize(tokens[0], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhIIN2oYuPFN"
      },
      "outputs": [],
      "source": [
        "# Decode the first article\n",
        "xart = xarts[1]['article']\n",
        "question = xart.numpy().decode()\n",
        "print(question[:512])\n",
        "\n",
        "tokenized = next(padding_fun(trax.data.tokenize([question,], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)))\n",
        "trax.data.detokenize(tokenized, vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)\n",
        "\n",
        "with trax.fastmath.use_backend(trax.fastmath.Backend.JAX):\n",
        "  model_predict.state = old_state\n",
        "  \n",
        "  tokens = autoregressive_sample(model_predict, tokenized[None,:1024], temperature=0., max_length=50)\n",
        "  print(tokens) \n",
        "  print(trax.data.detokenize(tokens[0], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ly0etXmuRW1"
      },
      "outputs": [],
      "source": [
        "# Decode the first article\n",
        "xart = xarts[2]['article']\n",
        "question = xart.numpy().decode()\n",
        "print(question[:512])\n",
        "\n",
        "tokenized = next(padding_fun(trax.data.tokenize([question,], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)))\n",
        "trax.data.detokenize(tokenized, vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100)\n",
        "\n",
        "with trax.fastmath.use_backend(trax.fastmath.Backend.JAX):\n",
        "  model_predict.state = old_state\n",
        "  \n",
        "  tokens = autoregressive_sample(model_predict, tokenized[None,:1024], temperature=0., max_length=50)\n",
        "  print(tokens) \n",
        "  print(trax.data.detokenize(tokens[0], vocab_file='all.16k.vocab', vocab_dir='//je-d/home/afrozm/rs=6.3/mira/data/v1/', n_reserved_ids=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0-Ip_VYMs3V"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "dataset = tfds.summarization.scientific_papers.ScientificPapers()\n",
        "valid = tfds.load(name='scientific_papers/arxiv:1.1.1')['test']\n",
        "index = 0\n",
        "for x in valid:\n",
        "  xart = x['article']\n",
        "  question = xart.numpy().decode()\n",
        "  print('========= Article {} ========'.format(index))\n",
        "  print(question[:15*1024])\n",
        "  index += 1\n",
        "  if index == 3:\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Short HighRAM Arxiv, ScalingTransformer.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "17pTV3shff9bQrgfmlxfKDs-iwA1LKlHr",
          "timestamp": 1625674094729
        },
        {
          "file_id": "1-RTUBF0TxUgVrtvqTSsCVEgUn6cRmOlk",
          "timestamp": 1625047803456
        },
        {
          "file_id": "1wKfNyPZ7h2ejEX3_x741oshTaGgc2zb0",
          "timestamp": 1624876859730
        },
        {
          "file_id": "1xdJt0cbCnC8FDyVckwwPzpHrxMTTFddR",
          "timestamp": 1624389640756
        },
        {
          "file_id": "1B6mc2s6AGVRVUO6OcgtKCT78EcYQIblu",
          "timestamp": 1624389519165
        },
        {
          "file_id": "1s8qohay75JCBDO24PdSWJvy6p5mb9_pF",
          "timestamp": 1623341957606
        },
        {
          "file_id": "1AzIkKG_NaiGLbBoJ-iTQP8D5Ijrgj_Ub",
          "timestamp": 1622738185436
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
